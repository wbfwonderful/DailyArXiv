---
title: Latest 15 Papers - June 05, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection](http://arxiv.org/abs/2506.02535v1)** | 2025-06-03 |  |
| **[AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection](http://arxiv.org/abs/2504.04495v2)** | 2025-05-31 | <details><summary>12 pa...</summary><p>12 pages, 6 figures, 9 tables. This work has been submitted to the IEEE for possible publication</p></details> |
| **[ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications](http://arxiv.org/abs/2505.02179v2)** | 2025-05-29 | <details><summary>A new...</summary><p>A newly identified systematic error in our data processing pipeline has affected the calculation and reporting of AUC metrics (notably in Tables [1, 2]). This significantly impacts our main experimental results and conclusions, compromising their reliability. To ensure academic rigor and prevent misleading information, this manuscript is withdrawn for thorough correction and re-evaluation</p></details> |
| **[Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought](http://arxiv.org/abs/2505.19877v1)** | 2025-05-26 | 9 pages, 4 figures |
| **[Rethinking Metrics and Benchmarks of Video Anomaly Detection](http://arxiv.org/abs/2505.19022v1)** | 2025-05-25 |  |
| **[MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation](http://arxiv.org/abs/2406.18815v3)** | 2025-05-23 | <details><summary>Accep...</summary><p>Accepted to WACV 2025</p></details> |
| **[Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection](http://arxiv.org/abs/2505.15205v2)** | 2025-05-23 | 12 pages, 5 figures |
| **[Just Dance with $Ï€$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection](http://arxiv.org/abs/2505.13123v1)** | 2025-05-19 |  |
| **[Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection](http://arxiv.org/abs/2505.02393v2)** | 2025-05-08 |  |
| **[Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous Driving](http://arxiv.org/abs/2406.06423v3)** | 2025-04-28 | <details><summary>Danie...</summary><p>Daniel Bogdoll and Jan Imhof contributed equally. Accepted for publication at BMVC 2024 RROW workshop. Won Best Paper Award</p></details> |
| **[Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches](http://arxiv.org/abs/2504.14753v1)** | 2025-04-20 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Image Processing (TIP)</p></details> |
| **[EventVAD: Training-Free Event-Aware Video Anomaly Detection](http://arxiv.org/abs/2504.13092v1)** | 2025-04-17 |  |
| **[SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model](http://arxiv.org/abs/2504.10320v1)** | 2025-04-14 |  |
| **[From Explicit Rules to Implicit Reasoning in Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2410.21991v6)** | 2025-04-06 | <details><summary>This ...</summary><p>This manuscript has been submitted to IEEE Transactions on Circuits and Systems for Video Technology and is under consideration for publication, with potential copyright transfer in the future</p></details> |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v4)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to ACM Computing Surveys. For more information and supplementary material, please visit https://github.com/fdjingliu/NSVAD</p></details> |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[EgoVLM: Policy Optimization for Egocentric Video Understanding](http://arxiv.org/abs/2506.03097v1)** | 2025-06-03 | <details><summary>Our C...</summary><p>Our Code can be found at https://github.com/adityavavre/VidEgoVLM</p></details> |
| **[HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](http://arxiv.org/abs/2506.02975v1)** | 2025-06-03 |  |
| **[METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding](http://arxiv.org/abs/2506.02850v1)** | 2025-06-03 | 14 pages, 10 figures |
| **[Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments](http://arxiv.org/abs/2506.02845v1)** | 2025-06-03 | <details><summary>15 pa...</summary><p>15 pages, 3 figures, submitted to NeurIPS 2025</p></details> |
| **[SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](http://arxiv.org/abs/2410.04417v4)** | 2025-06-03 | <details><summary>Accep...</summary><p>Accepted by ICML 2025</p></details> |
| **[StimuVAR: Spatiotemporal Stimuli-aware Video Affective Reasoning with Multimodal Large Language Models](http://arxiv.org/abs/2409.00304v2)** | 2025-06-03 | <details><summary>Paper...</summary><p>Paper is accepted by IJCV</p></details> |
| **[VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding](http://arxiv.org/abs/2501.13106v4)** | 2025-06-03 | <details><summary>BZ, K...</summary><p>BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3</p></details> |
| **[InterRVOS: Interaction-aware Referring Video Object Segmentation](http://arxiv.org/abs/2506.02356v1)** | 2025-06-03 |  |
| **[Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency](http://arxiv.org/abs/2506.01908v1)** | 2025-06-02 |  |
| **[MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning](http://arxiv.org/abs/2306.02252v3)** | 2025-06-02 |  |
| **[ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding](http://arxiv.org/abs/2506.01300v1)** | 2025-06-02 | 31 pages, 18 figures |
| **[MOOSE: Pay Attention to Temporal Dynamics for Video Understanding via Optical Flows](http://arxiv.org/abs/2506.01119v1)** | 2025-06-01 |  |
| **[FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](http://arxiv.org/abs/2506.00993v1)** | 2025-06-01 |  |
| **[Scene Detection Policies and Keyframe Extraction Strategies for Large-Scale Video Analysis](http://arxiv.org/abs/2506.00667v1)** | 2025-05-31 | <details><summary>24 pa...</summary><p>24 pages, 8 figures, submitted as a preprint. ArXiv preprint only, not submitted to a journal yet</p></details> |
| **[QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design](http://arxiv.org/abs/2505.16175v2)** | 2025-05-31 | <details><summary>19 pa...</summary><p>19 pages, 6 figures, 2 tables</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](http://arxiv.org/abs/2506.03135v1)** | 2025-06-03 | <details><summary>Proje...</summary><p>Project Page: https://qizekun.github.io/omnispatial/</p></details> |
| **[Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models](http://arxiv.org/abs/2503.16724v2)** | 2025-06-03 |  |
| **[EgoVLM: Policy Optimization for Egocentric Video Understanding](http://arxiv.org/abs/2506.03097v1)** | 2025-06-03 | <details><summary>Our C...</summary><p>Our Code can be found at https://github.com/adityavavre/VidEgoVLM</p></details> |
| **[DPO Learning with LLMs-Judge Signal for Computer Use Agents](http://arxiv.org/abs/2506.03095v1)** | 2025-06-03 |  |
| **[From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](http://arxiv.org/abs/2506.03093v1)** | 2025-06-03 | Preprint |
| **[Text-guided Generation of Efficient Personalized Inspection Plans](http://arxiv.org/abs/2506.02917v1)** | 2025-06-03 | 8 pages, 5 figures |
| **[FlySearch: Exploring how vision-language models explore](http://arxiv.org/abs/2506.02896v1)** | 2025-06-03 |  |
| **[Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](http://arxiv.org/abs/2506.02865v1)** | 2025-06-03 | Alphabetical order |
| **[Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection](http://arxiv.org/abs/2503.10080v3)** | 2025-06-03 |  |
| **[SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](http://arxiv.org/abs/2506.02803v1)** | 2025-06-03 |  |
| **[Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](http://arxiv.org/abs/2505.03792v2)** | 2025-06-03 | ICML 2025 |
| **[Learning from True-False Labels via Multi-modal Prompt Retrieving](http://arxiv.org/abs/2405.15228v2)** | 2025-06-03 | 15 pages, 5 figures |
| **[X-Driver: Explainable Autonomous Driving with Vision-Language Models](http://arxiv.org/abs/2505.05098v2)** | 2025-06-03 |  |
| **[Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning](http://arxiv.org/abs/2506.02738v1)** | 2025-06-03 | 15 pages |
| **[Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](http://arxiv.org/abs/2506.02708v1)** | 2025-06-03 | Accepted to ICIP2025 |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation](http://arxiv.org/abs/2506.03126v1)** | 2025-06-03 | <details><summary>Proje...</summary><p>Project released at: https://qiulu66.github.io/animeshooter/</p></details> |
| **[Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs](http://arxiv.org/abs/2502.11184v2)** | 2025-06-03 | Accepted by ACL 2025 |
| **[S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](http://arxiv.org/abs/2505.24139v2)** | 2025-06-03 | <details><summary>Accep...</summary><p>Accepted by CVPR2025; Project website: s4-driver.github.io</p></details> |
| **[InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts](http://arxiv.org/abs/2505.19028v3)** | 2025-06-03 |  |
| **[Evaluating and Advancing Multimodal Large Language Models in Perception Ability Lens](http://arxiv.org/abs/2411.14725v2)** | 2025-06-03 | <details><summary>Code ...</summary><p>Code repository: https://github.com/Chenfeng1271/AbilityLens/tree/main</p></details> |
| **[Scaling and Beyond: Advancing Spatial Reasoning in MLLMs Requires New Recipes](http://arxiv.org/abs/2504.15037v2)** | 2025-06-03 |  |
| **[Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM](http://arxiv.org/abs/2505.19901v3)** | 2025-06-03 |  |
| **[CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](http://arxiv.org/abs/2506.02544v1)** | 2025-06-03 | <details><summary>Accep...</summary><p>Accepted to ACL 2025 Main</p></details> |
| **[VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](http://arxiv.org/abs/2506.02537v1)** | 2025-06-03 | 13 pages, 4 figures |
| **[StimuVAR: Spatiotemporal Stimuli-aware Video Affective Reasoning with Multimodal Large Language Models](http://arxiv.org/abs/2409.00304v2)** | 2025-06-03 | <details><summary>Paper...</summary><p>Paper is accepted by IJCV</p></details> |
| **[Reinforcing Video Reasoning with Focused Thinking](http://arxiv.org/abs/2505.24718v2)** | 2025-06-03 |  |
| **[QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation](http://arxiv.org/abs/2506.02295v1)** | 2025-06-02 |  |
| **[SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](http://arxiv.org/abs/2502.12562v3)** | 2025-06-02 | <details><summary>Accep...</summary><p>Accepted in ACL 2025 Main Track</p></details> |
| **[From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](http://arxiv.org/abs/2506.02242v1)** | 2025-06-02 |  |
| **[MAPLE: A Mobile Agent with Persistent Finite State Machines for Structured Task Reasoning](http://arxiv.org/abs/2505.23596v2)** | 2025-06-02 | <details><summary>chang...</summary><p>change to a better title</p></details> |

