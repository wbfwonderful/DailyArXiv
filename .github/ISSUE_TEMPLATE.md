---
title: Latest 15 Papers - March 12, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](http://arxiv.org/abs/2503.04504v1)** | 2025-03-06 |  |
| **[Anomaly detection in non-stationary videos using time-recursive differencing network based prediction](http://arxiv.org/abs/2503.02234v1)** | 2025-03-04 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos](http://arxiv.org/abs/2503.00670v1)** | 2025-03-02 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](http://arxiv.org/abs/2410.01506v4)** | 2025-02-28 | <details><summary>Accep...</summary><p>Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM](http://arxiv.org/abs/2502.18863v1)** | 2025-02-26 |  |
| **[An Attribute-based Method for Video Anomaly Detection](http://arxiv.org/abs/2212.00789v2)** | 2025-01-26 | <details><summary>TMLR ...</summary><p>TMLR 2025. Our code is available at https://github.com/talreiss/Accurate-Interpretable-VAD</p></details> |
| **[Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2406.02831v2)** | 2025-01-18 |  |
| **[ComplexVAD: Detecting Interaction Anomalies in Video](http://arxiv.org/abs/2501.09733v1)** | 2025-01-16 | <details><summary>16 pa...</summary><p>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</p></details> |
| **[Detecting Contextual Anomalies by Discovering Consistent Spatial Regions](http://arxiv.org/abs/2501.08470v1)** | 2025-01-14 |  |
| **[Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive Knowledge Graph Learning](http://arxiv.org/abs/2411.09072v2)** | 2025-01-14 | <details><summary>Accep...</summary><p>Accepted to DATE 2025</p></details> |
| **[Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2412.20455v1)** | 2024-12-29 | <details><summary>Accep...</summary><p>Accepted to CVPR'24 MULA Workshop</p></details> |
| **[Exploring the Magnitude-Shape Plot Framework for Anomaly Detection in Crowded Video Scenes](http://arxiv.org/abs/2412.20363v1)** | 2024-12-29 | <details><summary>21 pa...</summary><p>21 pages, 4 figures, 10 tables</p></details> |
| **[Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems](http://arxiv.org/abs/2412.20201v1)** | 2024-12-28 | <details><summary>IEEE ...</summary><p>IEEE TETC-CS (Under review)</p></details> |
| **[STNMamba: Mamba-based Spatial-Temporal Normality Learning for Video Anomaly Detection](http://arxiv.org/abs/2412.20084v1)** | 2024-12-28 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[ALLVB: All-in-One Long Video Understanding Benchmark](http://arxiv.org/abs/2503.07298v1)** | 2025-03-10 | AAAI 2025 |
| **[MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos](http://arxiv.org/abs/2502.12558v2)** | 2025-03-10 |  |
| **[VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding](http://arxiv.org/abs/2403.14743v3)** | 2025-03-10 |  |
| **[VCA: Video Curious Agent for Long Video Understanding](http://arxiv.org/abs/2412.10471v2)** | 2025-03-10 |  |
| **[Towards Fine-Grained Video Question Answering](http://arxiv.org/abs/2503.06820v1)** | 2025-03-10 |  |
| **[TimeLoc: A Unified End-to-End Framework for Precise Timestamp Localization in Long Videos](http://arxiv.org/abs/2503.06526v1)** | 2025-03-09 | <details><summary>Code ...</summary><p>Code & models will be released at https://github.com/sming256/TimeLoc. The first 4 authors contributes equally</p></details> |
| **[Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs](http://arxiv.org/abs/2406.09367v3)** | 2025-03-07 | ICLR 2025 |
| **[Unified Reward Model for Multimodal Understanding and Generation](http://arxiv.org/abs/2503.05236v1)** | 2025-03-07 | <details><summary>proje...</summary><p>project page: https://codegoat24.github.io/UnifiedReward/</p></details> |
| **[Token-Efficient Long Video Understanding for Multimodal LLMs](http://arxiv.org/abs/2503.04130v1)** | 2025-03-06 |  |
| **[EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language Models](http://arxiv.org/abs/2503.04058v1)** | 2025-03-06 |  |
| **[Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection](http://arxiv.org/abs/2503.03562v2)** | 2025-03-06 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[EgoLife: Towards Egocentric Life Assistant](http://arxiv.org/abs/2503.03803v1)** | 2025-03-05 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025. Project Page: https://egolife-ai.github.io/. Code: https://github.com/EvolvingLMMs-Lab/EgoLife</p></details> |
| **[AVD2: Accident Video Diffusion for Accident Video Description](http://arxiv.org/abs/2502.14801v3)** | 2025-03-04 | <details><summary>ICRA ...</summary><p>ICRA 2025, Project Page: https://an-answer-tree.github.io/</p></details> |
| **[Frame-Voyager: Learning to Query Frames for Video Large Language Models](http://arxiv.org/abs/2410.03226v3)** | 2025-03-04 | <details><summary>ICLR ...</summary><p>ICLR 2025, Camera-ready Version</p></details> |
| **[TRACE: Temporal Grounding Video LLM via Causal Event Modeling](http://arxiv.org/abs/2410.05643v3)** | 2025-03-03 | ICLR 2025 |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model](http://arxiv.org/abs/2406.20076v5)** | 2025-03-10 | <details><summary>Prepr...</summary><p>Preprint. Update: (1) better performance and (2) versatile segmentation. Code and models are available at: https://github.com/hustvl/EVF-SAM</p></details> |
| **[Mask-Adapter: The Devil is in the Masks for Open-Vocabulary Segmentation](http://arxiv.org/abs/2412.04533v2)** | 2025-03-10 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025; Code & models: https://github.com/hustvl/MaskAdapter</p></details> |
| **[FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates](http://arxiv.org/abs/2503.07216v1)** | 2025-03-10 | Preprint |
| **[Narrating the Video: Boosting Text-Video Retrieval via Comprehensive Utilization of Frame-Level Captions](http://arxiv.org/abs/2503.05186v2)** | 2025-03-10 | <details><summary>Accep...</summary><p>Accepted at CVPR 2025</p></details> |
| **[Learning A Zero-shot Occupancy Network from Vision Foundation Models via Self-supervised Adaptation](http://arxiv.org/abs/2503.07125v1)** | 2025-03-10 | preprint |
| **[XR-VLM: Cross-Relationship Modeling with Multi-part Prompts and Visual Features for Fine-Grained Recognition](http://arxiv.org/abs/2503.07075v1)** | 2025-03-10 |  |
| **[Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning](http://arxiv.org/abs/2503.07065v1)** | 2025-03-10 |  |
| **[RoboReflect: A Robotic Reflective Reasoning Framework for Grasping Ambiguous-Condition Objects](http://arxiv.org/abs/2501.09307v2)** | 2025-03-10 |  |
| **[DiagramQG: Concept-Focused Diagram Question Generation via Hierarchical Knowledge Integration](http://arxiv.org/abs/2411.17771v3)** | 2025-03-10 |  |
| **[Unbiased Region-Language Alignment for Open-Vocabulary Dense Prediction](http://arxiv.org/abs/2412.06244v2)** | 2025-03-10 |  |
| **[CAPT: Class-Aware Prompt Tuning for Federated Long-Tailed Learning with Vision-Language Model](http://arxiv.org/abs/2503.06993v1)** | 2025-03-10 |  |
| **[A Multimodal Benchmark Dataset and Model for Crop Disease Diagnosis](http://arxiv.org/abs/2503.06973v1)** | 2025-03-10 | <details><summary>Accep...</summary><p>Accepted by ECCV 2024 (14 pages, 8 figures)</p></details> |
| **[Robi Butler: Multimodal Remote Interaction with a Household Robot Assistant](http://arxiv.org/abs/2409.20548v2)** | 2025-03-10 | <details><summary>Accep...</summary><p>Accepted to ICRA 2025</p></details> |
| **[Handle Object Navigation as Weighted Traveling Repairman Problem](http://arxiv.org/abs/2503.06937v1)** | 2025-03-10 |  |
| **[Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual Learning Vision-Language Models with Dynamic Rank-Selective LoRA](http://arxiv.org/abs/2412.01004v4)** | 2025-03-10 | Preprint |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?](http://arxiv.org/abs/2503.07487v1)** | 2025-03-10 |  |
| **[REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding](http://arxiv.org/abs/2503.07413v1)** | 2025-03-10 |  |
| **[Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation](http://arxiv.org/abs/2501.19017v2)** | 2025-03-10 |  |
| **[A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language Models with Fundus Photographs and OCT Images](http://arxiv.org/abs/2503.07094v1)** | 2025-03-10 |  |
| **[Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning](http://arxiv.org/abs/2503.07002v1)** | 2025-03-10 |  |
| **[R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning](http://arxiv.org/abs/2503.05379v2)** | 2025-03-10 |  |
| **[Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](http://arxiv.org/abs/2503.06989v1)** | 2025-03-10 |  |
| **[Lightweight Multimodal Artificial Intelligence Framework for Maritime Multi-Scene Recognition](http://arxiv.org/abs/2503.06978v1)** | 2025-03-10 | <details><summary>19 pa...</summary><p>19 pages, 4 figures, submitted to Engineering Applications of Artificial Intelligence</p></details> |
| **[MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos](http://arxiv.org/abs/2502.12558v2)** | 2025-03-10 |  |
| **[ProBench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks](http://arxiv.org/abs/2503.06885v1)** | 2025-03-10 |  |
| **[PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks](http://arxiv.org/abs/2503.04065v2)** | 2025-03-10 |  |
| **[Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models](http://arxiv.org/abs/2503.06749v1)** | 2025-03-09 |  |
| **[From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities](http://arxiv.org/abs/2410.02155v3)** | 2025-03-09 |  |
| **[ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy](http://arxiv.org/abs/2503.06542v1)** | 2025-03-09 |  |
| **[VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling](http://arxiv.org/abs/2501.00574v3)** | 2025-03-09 |  |

