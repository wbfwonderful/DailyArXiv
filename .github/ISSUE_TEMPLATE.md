---
title: Latest 15 Papers - April 01, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis](http://arxiv.org/abs/2503.21904v1)** | 2025-03-27 | 13 pages |
| **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)** | 2025-03-27 | <details><summary>Accpe...</summary><p>Accpeted by ICME 2025</p></details> |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)** | 2025-03-27 |  |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v2)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted in CVPR 2025</p></details> |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v3)** | 2025-03-26 | <details><summary>Revis...</summary><p>Revised to ACM Computing Surveys, under review, for more information and supplementary material, please see https://github.com/fdjingliu/NSVAD</p></details> |
| **[Video Anomaly Detection with Contours -- A Study](http://arxiv.org/abs/2503.19588v1)** | 2025-03-25 |  |
| **[CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2503.18808v1)** | 2025-03-24 | <details><summary>Accep...</summary><p>Accepted for publication by IEEE Transactions on Image Processing</p></details> |
| **[Anomize: Better Open Vocabulary Video Anomaly Detection](http://arxiv.org/abs/2503.18094v1)** | 2025-03-23 |  |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |
| **[Language-guided Open-world Video Anomaly Detection](http://arxiv.org/abs/2503.13160v1)** | 2025-03-17 |  |
| **[UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks](http://arxiv.org/abs/2503.12905v1)** | 2025-03-17 | <details><summary>Accep...</summary><p>Accepted by AAAI 2025</p></details> |
| **[Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos](http://arxiv.org/abs/2503.07799v1)** | 2025-03-10 |  |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Structured Keywords](http://arxiv.org/abs/2503.10653v1)** | 2025-03-07 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](http://arxiv.org/abs/2503.12496v2)** | 2025-03-28 |  |
| **[Frame-Voyager: Learning to Query Frames for Video Large Language Models](http://arxiv.org/abs/2410.03226v4)** | 2025-03-28 | <details><summary>ICLR ...</summary><p>ICLR 2025, Camera-ready Version</p></details> |
| **[ReWind: Understanding Long Videos with Instructed Learnable Memory](http://arxiv.org/abs/2411.15556v2)** | 2025-03-27 |  |
| **[Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model](http://arxiv.org/abs/2503.21782v1)** | 2025-03-27 | <details><summary>Techn...</summary><p>Technical Report. Project Page: https://amshaker.github.io/Mobile-VideoGPT</p></details> |
| **[OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](http://arxiv.org/abs/2501.05510v2)** | 2025-03-27 | CVPR 2025 |
| **[SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](http://arxiv.org/abs/2503.18943v2)** | 2025-03-27 | Technical report |
| **[BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](http://arxiv.org/abs/2503.21483v1)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **[LongViTU: Instruction Tuning for Long-Form Video Understanding](http://arxiv.org/abs/2501.05037v2)** | 2025-03-27 |  |
| **[M-LLM Based Video Frame Selection for Efficient Video Understanding](http://arxiv.org/abs/2502.19680v2)** | 2025-03-26 |  |
| **[From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment](http://arxiv.org/abs/2503.20472v1)** | 2025-03-26 |  |
| **[Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video Understanding](http://arxiv.org/abs/2503.20362v1)** | 2025-03-26 |  |
| **[Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection](http://arxiv.org/abs/2503.03562v3)** | 2025-03-26 | Accepted by CVPR25 |
| **[Progress-Aware Video Frame Captioning](http://arxiv.org/abs/2412.02071v2)** | 2025-03-26 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025, Project website: https://vision.cs.utexas.edu/projects/ProgressCaptioner/</p></details> |
| **[HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding](http://arxiv.org/abs/2501.01645v2)** | 2025-03-26 | <details><summary>Accep...</summary><p>Accepted to ICME 2025</p></details> |
| **[LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living](http://arxiv.org/abs/2406.09390v3)** | 2025-03-25 | <details><summary>CVPR ...</summary><p>CVPR 2025 Camera Ready</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Understanding Co-speech Gestures in-the-wild](http://arxiv.org/abs/2503.22668v1)** | 2025-03-28 | <details><summary>Main ...</summary><p>Main paper - 11 pages, 4 figures, Supplementary - 5 pages, 4 figures</p></details> |
| **[Unicorn: Text-Only Data Synthesis for Vision Language Model Training](http://arxiv.org/abs/2503.22655v1)** | 2025-03-28 |  |
| **[TULIP: Token-length Upgraded CLIP](http://arxiv.org/abs/2410.10034v2)** | 2025-03-28 |  |
| **[Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding](http://arxiv.org/abs/2503.16707v2)** | 2025-03-28 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis](http://arxiv.org/abs/2503.14536v2)** | 2025-03-28 | 10 pages , 3 figures |
| **[VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection](http://arxiv.org/abs/2503.22291v1)** | 2025-03-28 | 5 pages, 4 figures |
| **[FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning](http://arxiv.org/abs/2503.22263v1)** | 2025-03-28 | <details><summary>https...</summary><p>https://github.com/0-ml/flip</p></details> |
| **[REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation](http://arxiv.org/abs/2503.22122v1)** | 2025-03-28 |  |
| **[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](http://arxiv.org/abs/2503.12496v2)** | 2025-03-28 |  |
| **[AnyAttack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models](http://arxiv.org/abs/2410.05346v3)** | 2025-03-28 | CVPR 2025 |
| **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v3)** | 2025-03-28 |  |
| **[How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](http://arxiv.org/abs/2503.22093v1)** | 2025-03-28 | <details><summary>2 pag...</summary><p>2 pages, accepted by ToM@AAAI25</p></details> |
| **[A Survey on Remote Sensing Foundation Models: From Vision to Multimodality](http://arxiv.org/abs/2503.22081v1)** | 2025-03-28 |  |
| **[ReWind: Understanding Long Videos with Instructed Learnable Memory](http://arxiv.org/abs/2411.15556v2)** | 2025-03-27 |  |
| **[CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models](http://arxiv.org/abs/2503.22020v1)** | 2025-03-27 | <details><summary>Proje...</summary><p>Project website: https://cot-vla.github.io/</p></details> |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models](http://arxiv.org/abs/2410.13360v3)** | 2025-03-28 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025. Code: https://github.com/Hoar012/RAP-MLLM</p></details> |
| **[Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](http://arxiv.org/abs/2503.18172v2)** | 2025-03-28 | <details><summary>31 pa...</summary><p>31 pages in total. Under Review For ARR</p></details> |
| **[Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users](http://arxiv.org/abs/2503.22610v1)** | 2025-03-28 |  |
| **[DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](http://arxiv.org/abs/2503.19498v2)** | 2025-03-28 | 87 pages, 65 figures |
| **[OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning](http://arxiv.org/abs/2503.16081v2)** | 2025-03-28 |  |
| **[EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos](http://arxiv.org/abs/2503.22152v1)** | 2025-03-28 |  |
| **[Tokenization of Gaze Data](http://arxiv.org/abs/2503.22145v1)** | 2025-03-28 |  |
| **[Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](http://arxiv.org/abs/2405.12523v3)** | 2025-03-28 |  |
| **[Do Multimodal Large Language Models See Like Humans?](http://arxiv.org/abs/2412.09603v2)** | 2025-03-27 | <details><summary>Proje...</summary><p>Project page: https://jiaying.link/HVSBench/</p></details> |
| **[Video-R1: Reinforcing Video Reasoning in MLLMs](http://arxiv.org/abs/2503.21776v1)** | 2025-03-27 | <details><summary>Proje...</summary><p>Project page: https://github.com/tulerfeng/Video-R1</p></details> |
| **[OmniBench: Towards The Future of Universal Omni-Language Models](http://arxiv.org/abs/2409.15272v4)** | 2025-03-27 |  |
| **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v1)** | 2025-03-27 |  |
| **[FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs](http://arxiv.org/abs/2503.21457v1)** | 2025-03-27 | Accepted by CVPR2025 |
| **[Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](http://arxiv.org/abs/2412.00493v2)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models](http://arxiv.org/abs/2405.15668v4)** | 2025-03-27 |  |

