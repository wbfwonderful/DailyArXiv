---
title: Latest 15 Papers - April 17, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model](http://arxiv.org/abs/2504.10320v1)** | 2025-04-14 |  |
| **[AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection](http://arxiv.org/abs/2504.04495v1)** | 2025-04-06 | <details><summary>11 pa...</summary><p>11 pages, 4 figures, 6 tables</p></details> |
| **[From Explicit Rules to Implicit Reasoning in Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2410.21991v6)** | 2025-04-06 | <details><summary>This ...</summary><p>This manuscript has been submitted to IEEE Transactions on Circuits and Systems for Video Technology and is under consideration for publication, with potential copyright transfer in the future</p></details> |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v4)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to ACM Computing Surveys. For more information and supplementary material, please visit https://github.com/fdjingliu/NSVAD</p></details> |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v3)** | 2025-03-31 | <details><summary>Accep...</summary><p>Accepted in CVPR 2025</p></details> |
| **[AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis](http://arxiv.org/abs/2503.21904v1)** | 2025-03-27 | 13 pages |
| **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)** | 2025-03-27 | <details><summary>Accpe...</summary><p>Accpeted by ICME 2025</p></details> |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)** | 2025-03-27 |  |
| **[Video Anomaly Detection with Contours -- A Study](http://arxiv.org/abs/2503.19588v1)** | 2025-03-25 |  |
| **[CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2503.18808v1)** | 2025-03-24 | <details><summary>Accep...</summary><p>Accepted for publication by IEEE Transactions on Image Processing</p></details> |
| **[Anomize: Better Open Vocabulary Video Anomaly Detection](http://arxiv.org/abs/2503.18094v1)** | 2025-03-23 |  |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |
| **[Language-guided Open-world Video Anomaly Detection](http://arxiv.org/abs/2503.13160v1)** | 2025-03-17 |  |
| **[UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks](http://arxiv.org/abs/2503.12905v1)** | 2025-03-17 | <details><summary>Accep...</summary><p>Accepted by AAAI 2025</p></details> |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild](http://arxiv.org/abs/2504.11326v1)** | 2025-04-15 | <details><summary>Works...</summary><p>Workshop Page: https://pvuw.github.io/. arXiv admin note: text overlap with arXiv:2504.00476, arXiv:2504.05178</p></details> |
| **[F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos](http://arxiv.org/abs/2504.08222v2)** | 2025-04-15 | <details><summary>ICLR ...</summary><p>ICLR 2025; Website URL: https://lzyandy.github.io/f3set-website/</p></details> |
| **[OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding](http://arxiv.org/abs/2504.10825v1)** | 2025-04-15 | <details><summary>Our p...</summary><p>Our project page: https://tele-ai.github.io/OmniVDiff/</p></details> |
| **[Multimodal Long Video Modeling Based on Temporal Dynamic Context](http://arxiv.org/abs/2504.10443v1)** | 2025-04-14 |  |
| **[Mavors: Multi-granularity Video Representation for Multimodal Large Language Model](http://arxiv.org/abs/2504.10068v1)** | 2025-04-14 | 22 pages |
| **[TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning](http://arxiv.org/abs/2504.09641v1)** | 2025-04-13 |  |
| **[MM-Ego: Towards Building Egocentric Multimodal LLMs for Video QA](http://arxiv.org/abs/2410.07177v2)** | 2025-04-13 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025</p></details> |
| **[VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning](http://arxiv.org/abs/2504.06958v3)** | 2025-04-13 |  |
| **[VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation Models Beat GPT-4o & Gemini-1.5 Pro](http://arxiv.org/abs/2504.09282v1)** | 2025-04-12 |  |
| **[Towards Efficient and Robust Moment Retrieval System: A Unified Framework for Multi-Granularity Models and Temporal Reranking](http://arxiv.org/abs/2504.08384v1)** | 2025-04-11 |  |
| **[SpaceVLLM: Endowing Multimodal Large Language Model with Spatio-Temporal Video Grounding Capability](http://arxiv.org/abs/2503.13983v3)** | 2025-04-11 |  |
| **[SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding](http://arxiv.org/abs/2504.07745v1)** | 2025-04-10 | Accepted to CVPR2025 |
| **[VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding](http://arxiv.org/abs/2504.07519v1)** | 2025-04-10 |  |
| **[How Can Objects Help Video-Language Understanding?](http://arxiv.org/abs/2504.07454v1)** | 2025-04-10 |  |
| **[LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding](http://arxiv.org/abs/2504.06835v1)** | 2025-04-09 |  |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Kimi-VL Technical Report](http://arxiv.org/abs/2504.07491v2)** | 2025-04-15 |  |
| **[Breaking the Data Barrier -- Building GUI Agents Through Task Generalization](http://arxiv.org/abs/2504.10127v2)** | 2025-04-15 | 24 pages, 11 figures |
| **[From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation](http://arxiv.org/abs/2504.11368v1)** | 2025-04-15 | 10 pages, 5 figures |
| **[UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](http://arxiv.org/abs/2504.11257v1)** | 2025-04-15 |  |
| **[GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents](http://arxiv.org/abs/2504.10458v2)** | 2025-04-15 |  |
| **[R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning](http://arxiv.org/abs/2504.11195v1)** | 2025-04-15 | CVPR 2025 |
| **[Benchmarking Vision Language Models on German Factual Data](http://arxiv.org/abs/2504.11108v1)** | 2025-04-15 |  |
| **[Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR](http://arxiv.org/abs/2504.11101v1)** | 2025-04-15 |  |
| **[MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness](http://arxiv.org/abs/2312.04960v4)** | 2025-04-15 |  |
| **[Causal Graphical Models for Vision-Language Compositional Understanding](http://arxiv.org/abs/2412.09353v2)** | 2025-04-15 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025</p></details> |
| **[QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](http://arxiv.org/abs/2504.11038v1)** | 2025-04-15 | <details><summary>Accep...</summary><p>Accepted by NAACL 2025 main</p></details> |
| **[Fine-Tuning Florence2 for Enhanced Object Detection in Un-constructed Environments: Vision-Language Model Approach](http://arxiv.org/abs/2503.04918v4)** | 2025-04-15 | <details><summary>22 pa...</summary><p>22 pages, 13 Figures, 6 Tables</p></details> |
| **[Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation](http://arxiv.org/abs/2504.03193v2)** | 2025-04-15 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025 (Highlight)</p></details> |
| **[Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles](http://arxiv.org/abs/2504.10873v1)** | 2025-04-15 |  |
| **[LVLM_CSP: Accelerating Large Vision Language Models via Clustering, Scattering, and Pruning for Reasoning Segmentation](http://arxiv.org/abs/2504.10854v1)** | 2025-04-15 |  |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models](http://arxiv.org/abs/2504.10479v2)** | 2025-04-15 | Technical Report |
| **[Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](http://arxiv.org/abs/2503.18172v3)** | 2025-04-15 | <details><summary>31 pa...</summary><p>31 pages in total. Under Review</p></details> |
| **[SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial Attack](http://arxiv.org/abs/2410.02240v5)** | 2025-04-15 |  |
| **[VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge](http://arxiv.org/abs/2504.10342v2)** | 2025-04-15 | 56 pages, 43 figures |
| **[What Is a Good Caption? A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness](http://arxiv.org/abs/2502.14914v2)** | 2025-04-15 |  |
| **[TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models](http://arxiv.org/abs/2504.09897v2)** | 2025-04-15 | Preprint |
| **[Dopamine Audiobook: A Training-free MLLM Agent for Emotional and Human-like Audiobook Generation](http://arxiv.org/abs/2504.11002v1)** | 2025-04-15 |  |
| **[Muse: A Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles](http://arxiv.org/abs/2412.18416v2)** | 2025-04-15 |  |
| **[PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware Multi-Instance Video Preference Learning](http://arxiv.org/abs/2504.05810v2)** | 2025-04-15 |  |
| **[IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities](http://arxiv.org/abs/2408.12902v2)** | 2025-04-15 | AAAI 2025 |
| **[CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates](http://arxiv.org/abs/2504.10738v1)** | 2025-04-14 | <details><summary>Kun J...</summary><p>Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author. The main paper and supplementary material are both included here, total 23 pages (main paper is 10 pages and supplementary material is 13 pages), total 17 figures (6 figures in main paper and 11 figures in supplementary material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code will be available at https://Ankit-Zefan.github.io/CleanMap/</p></details> |
| **[Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization](http://arxiv.org/abs/2504.10727v1)** | 2025-04-14 | <details><summary>26 pa...</summary><p>26 pages, CVPR MORSE Workshop 2025</p></details> |
| **[MIEB: Massive Image Embedding Benchmark](http://arxiv.org/abs/2504.10471v1)** | 2025-04-14 |  |
| **[Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding](http://arxiv.org/abs/2504.10465v1)** | 2025-04-14 |  |
| **[The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer](http://arxiv.org/abs/2504.10462v1)** | 2025-04-14 |  |

