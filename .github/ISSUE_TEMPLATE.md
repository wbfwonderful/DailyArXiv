---
title: Latest 15 Papers - April 07, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v4)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to ACM Computing Surveys. For more information and supplementary material, please visit https://github.com/fdjingliu/NSVAD</p></details> |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v3)** | 2025-03-31 | <details><summary>Accep...</summary><p>Accepted in CVPR 2025</p></details> |
| **[AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis](http://arxiv.org/abs/2503.21904v1)** | 2025-03-27 | 13 pages |
| **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)** | 2025-03-27 | <details><summary>Accpe...</summary><p>Accpeted by ICME 2025</p></details> |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)** | 2025-03-27 |  |
| **[Video Anomaly Detection with Contours -- A Study](http://arxiv.org/abs/2503.19588v1)** | 2025-03-25 |  |
| **[CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2503.18808v1)** | 2025-03-24 | <details><summary>Accep...</summary><p>Accepted for publication by IEEE Transactions on Image Processing</p></details> |
| **[Anomize: Better Open Vocabulary Video Anomaly Detection](http://arxiv.org/abs/2503.18094v1)** | 2025-03-23 |  |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |
| **[Language-guided Open-world Video Anomaly Detection](http://arxiv.org/abs/2503.13160v1)** | 2025-03-17 |  |
| **[UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks](http://arxiv.org/abs/2503.12905v1)** | 2025-03-17 | <details><summary>Accep...</summary><p>Accepted by AAAI 2025</p></details> |
| **[Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos](http://arxiv.org/abs/2503.07799v1)** | 2025-03-10 |  |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Structured Keywords](http://arxiv.org/abs/2503.10653v1)** | 2025-03-07 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation](http://arxiv.org/abs/2412.09754v3)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025. Project page: https://ali2500.github.io/vicas-project/</p></details> |
| **[Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](http://arxiv.org/abs/2504.02438v1)** | 2025-04-03 |  |
| **[Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval](http://arxiv.org/abs/2504.02397v1)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **[Moment Quantization for Video Temporal Grounding](http://arxiv.org/abs/2504.02286v1)** | 2025-04-03 |  |
| **[Re-thinking Temporal Search for Long-Form Video Understanding](http://arxiv.org/abs/2504.02259v1)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025; A real-world long video needle-in-haystack benchmark; long-video QA with human ref frames</p></details> |
| **[Aligned Better, Listen Better for Audio-Visual Large Language Models](http://arxiv.org/abs/2504.02061v1)** | 2025-04-02 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[Is Temporal Prompting All We Need For Limited Labeled Action Recognition?](http://arxiv.org/abs/2504.01890v1)** | 2025-04-02 | <details><summary>Accep...</summary><p>Accepted in CVPR-W 2025</p></details> |
| **[Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning](http://arxiv.org/abs/2504.01805v1)** | 2025-04-02 |  |
| **[TimeSearch: Hierarchical Video Search with Spotlight and Reflection for Human-like Long Video Understanding](http://arxiv.org/abs/2504.01407v1)** | 2025-04-02 |  |
| **[Slow-Fast Architecture for Video Multi-Modal Large Language Models](http://arxiv.org/abs/2504.01328v1)** | 2025-04-02 | Technical report |
| **[Shot-by-Shot: Film-Grammar-Aware Training-Free Audio Description Generation](http://arxiv.org/abs/2504.01020v1)** | 2025-04-01 | <details><summary>Proje...</summary><p>Project Page: https://www.robots.ox.ac.uk/vgg/research/shot-by-shot/</p></details> |
| **[ALLVB: All-in-One Long Video Understanding Benchmark](http://arxiv.org/abs/2503.07298v2)** | 2025-04-01 | AAAI 2025 |
| **[VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](http://arxiv.org/abs/2503.13444v2)** | 2025-04-01 | <details><summary>Proje...</summary><p>Project Page: https://videomind.github.io/</p></details> |
| **[LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents](http://arxiv.org/abs/2503.10200v2)** | 2025-04-01 |  |
| **[VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding](http://arxiv.org/abs/2412.03735v2)** | 2025-03-31 | CVPR 2025 |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models](http://arxiv.org/abs/2405.05256v2)** | 2025-04-03 | <details><summary>In CV...</summary><p>In CVPR 2024. Code https://github.com/amazon-science/THRONE</p></details> |
| **[STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection](http://arxiv.org/abs/2504.02823v1)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted at CVPR 2025</p></details> |
| **[Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](http://arxiv.org/abs/2504.02821v1)** | 2025-04-03 | <details><summary>Prepr...</summary><p>Preprint. The code is available at https://github.com/ExplainableML/sae-for-vlm</p></details> |
| **[Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence](http://arxiv.org/abs/2504.02799v1)** | 2025-04-03 |  |
| **[Robot-Led Vision Language Model Wellbeing Assessment of Children](http://arxiv.org/abs/2504.02765v1)** | 2025-04-03 |  |
| **[Understanding Depth and Height Perception in Large Visual-Language Models](http://arxiv.org/abs/2408.11748v4)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted in CVPRW 2025</p></details> |
| **[Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme](http://arxiv.org/abs/2504.02587v1)** | 2025-04-03 | <details><summary>Code ...</summary><p>Code is public and available at: https://github.com/GAIR-NLP/MAYE</p></details> |
| **[Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision](http://arxiv.org/abs/2504.02477v1)** | 2025-04-03 | <details><summary>27 pa...</summary><p>27 pages, 11 figures, survey paper submitted to Information Fusion</p></details> |
| **[R+X: Retrieval and Execution from Everyday Human Videos](http://arxiv.org/abs/2407.12957v2)** | 2025-04-03 | <details><summary>Publi...</summary><p>Published at the IEEE International Conference on Robotics and Automation (ICRA) 2025</p></details> |
| **[Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](http://arxiv.org/abs/2504.02438v1)** | 2025-04-03 |  |
| **[ChatGarment: Garment Estimation, Generation and Editing via Large Language Models](http://arxiv.org/abs/2412.17811v3)** | 2025-04-03 | CVPR 2025 |
| **[Mixtera: A Data Plane for Foundation Model Training](http://arxiv.org/abs/2502.19790v2)** | 2025-04-03 | under submission |
| **[Evolving from Single-modal to Multi-modal Facial Deepfake Detection: Progress and Challenges](http://arxiv.org/abs/2406.06965v4)** | 2025-04-03 | <details><summary>P. Li...</summary><p>P. Liu is with the Department of Computer Science and Engineering, University of Nevada, Reno, NV, 89512. Q. Tao and J. Zhou are with Centre for Frontier AI Research (CFAR), and Institute of High Performance Computing (IHPC), A*STAR, Singapore. J. Zhou is also with Centre for Advanced Technologies in Online Safety (CATOS), A*STAR, Singapore. J. Zhou is the corresponding author</p></details> |
| **[ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active Feedback](http://arxiv.org/abs/2504.02357v1)** | 2025-04-03 | 13 pages, 5 figures |
| **[Large (Vision) Language Models are Unsupervised In-Context Learners](http://arxiv.org/abs/2504.02349v1)** | 2025-04-03 | <details><summary>ICLR ...</summary><p>ICLR 2025 camera-ready</p></details> |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation](http://arxiv.org/abs/2412.09754v3)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025. Project page: https://ali2500.github.io/vicas-project/</p></details> |
| **[Expertized Caption Auto-Enhancement for Video-Text Retrieval](http://arxiv.org/abs/2502.02885v2)** | 2025-04-03 |  |
| **[Evolving from Single-modal to Multi-modal Facial Deepfake Detection: Progress and Challenges](http://arxiv.org/abs/2406.06965v4)** | 2025-04-03 | <details><summary>P. Li...</summary><p>P. Liu is with the Department of Computer Science and Engineering, University of Nevada, Reno, NV, 89512. Q. Tao and J. Zhou are with Centre for Frontier AI Research (CFAR), and Institute of High Performance Computing (IHPC), A*STAR, Singapore. J. Zhou is also with Centre for Advanced Technologies in Online Safety (CATOS), A*STAR, Singapore. J. Zhou is the corresponding author</p></details> |
| **[The Plot Thickens: Quantitative Part-by-Part Exploration of MLLM Visualization Literacy](http://arxiv.org/abs/2504.02217v1)** | 2025-04-03 | 11 pages, 8 figures |
| **[Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities](http://arxiv.org/abs/2504.01954v1)** | 2025-04-02 |  |
| **[Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning](http://arxiv.org/abs/2503.15558v2)** | 2025-04-02 |  |
| **[Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications](http://arxiv.org/abs/2408.11878v2)** | 2025-04-02 | 33 pages, 13 figures |
| **[CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation](http://arxiv.org/abs/2406.10462v3)** | 2025-04-02 | <details><summary>22 pa...</summary><p>22 pages, Accepted by CVPR 2025</p></details> |
| **[Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources](http://arxiv.org/abs/2504.00595v2)** | 2025-04-02 |  |
| **[ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction](http://arxiv.org/abs/2504.01472v1)** | 2025-04-02 | <details><summary>Compu...</summary><p>Computer Vision and Pattern Recognition</p></details> |
| **[PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization](http://arxiv.org/abs/2504.01444v1)** | 2025-04-02 |  |
| **[Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning](http://arxiv.org/abs/2504.00907v2)** | 2025-04-02 |  |
| **[Olympus: A Universal Task Router for Computer Vision Tasks](http://arxiv.org/abs/2412.09612v3)** | 2025-04-01 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025, Project webpage: http://yuanze-lin.me/Olympus_page/</p></details> |
| **[Hallucination of Multimodal Large Language Models: A Survey](http://arxiv.org/abs/2404.18930v2)** | 2025-04-01 | 228 references |
| **[AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction](http://arxiv.org/abs/2504.01014v1)** | 2025-04-01 | <details><summary>Proje...</summary><p>Project released at: https://howe125.github.io/AnimeGamer.github.io/</p></details> |

