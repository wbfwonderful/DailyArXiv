---
title: Latest 15 Papers - March 17, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos](http://arxiv.org/abs/2503.07799v1)** | 2025-03-10 |  |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](http://arxiv.org/abs/2503.04504v1)** | 2025-03-06 |  |
| **[Anomaly detection in non-stationary videos using time-recursive differencing network based prediction](http://arxiv.org/abs/2503.02234v1)** | 2025-03-04 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos](http://arxiv.org/abs/2503.00670v1)** | 2025-03-02 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](http://arxiv.org/abs/2410.01506v4)** | 2025-02-28 | <details><summary>Accep...</summary><p>Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM](http://arxiv.org/abs/2502.18863v1)** | 2025-02-26 |  |
| **[An Attribute-based Method for Video Anomaly Detection](http://arxiv.org/abs/2212.00789v2)** | 2025-01-26 | <details><summary>TMLR ...</summary><p>TMLR 2025. Our code is available at https://github.com/talreiss/Accurate-Interpretable-VAD</p></details> |
| **[Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2406.02831v2)** | 2025-01-18 |  |
| **[ComplexVAD: Detecting Interaction Anomalies in Video](http://arxiv.org/abs/2501.09733v1)** | 2025-01-16 | <details><summary>16 pa...</summary><p>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</p></details> |
| **[Detecting Contextual Anomalies by Discovering Consistent Spatial Regions](http://arxiv.org/abs/2501.08470v1)** | 2025-01-14 |  |
| **[Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive Knowledge Graph Learning](http://arxiv.org/abs/2411.09072v2)** | 2025-01-14 | <details><summary>Accep...</summary><p>Accepted to DATE 2025</p></details> |
| **[Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2412.20455v1)** | 2024-12-29 | <details><summary>Accep...</summary><p>Accepted to CVPR'24 MULA Workshop</p></details> |
| **[Exploring the Magnitude-Shape Plot Framework for Anomaly Detection in Crowded Video Scenes](http://arxiv.org/abs/2412.20363v1)** | 2024-12-29 | <details><summary>21 pa...</summary><p>21 pages, 4 figures, 10 tables</p></details> |
| **[Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems](http://arxiv.org/abs/2412.20201v1)** | 2024-12-28 | <details><summary>IEEE ...</summary><p>IEEE TETC-CS (Under review)</p></details> |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Are you Struggling? Dataset and Baselines for Struggle Determination in Assembly Videos](http://arxiv.org/abs/2402.11057v4)** | 2025-03-13 |  |
| **[FaVChat: Unlocking Fine-Grained Facial Video Understanding with Multimodal Large Language Models](http://arxiv.org/abs/2503.09158v2)** | 2025-03-13 |  |
| **[LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents](http://arxiv.org/abs/2503.10200v1)** | 2025-03-13 |  |
| **[TIME: Temporal-sensitive Multi-dimensional Instruction Tuning and Benchmarking for Video-LLMs](http://arxiv.org/abs/2503.09994v1)** | 2025-03-13 |  |
| **[On the Limitations of Vision-Language Models in Understanding Image Transforms](http://arxiv.org/abs/2503.09837v1)** | 2025-03-12 | 8 pages, 15 images |
| **[Teaching VLMs to Localize Specific Objects from In-context Examples](http://arxiv.org/abs/2411.13317v2)** | 2025-03-12 |  |
| **[VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary](http://arxiv.org/abs/2503.09402v1)** | 2025-03-12 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025. Github: https://github.com/showlab/VLog</p></details> |
| **[VideoScan: Enabling Efficient Streaming Video Understanding via Frame-level Semantic Carriers](http://arxiv.org/abs/2503.09387v1)** | 2025-03-12 | 11 pages, 4 figures |
| **[Memory-enhanced Retrieval Augmentation for Long Video Understanding](http://arxiv.org/abs/2503.09149v1)** | 2025-03-12 |  |
| **[Generative Frame Sampler for Long Video Understanding](http://arxiv.org/abs/2503.09146v1)** | 2025-03-12 |  |
| **[Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding](http://arxiv.org/abs/2503.09143v1)** | 2025-03-12 | <details><summary>Proje...</summary><p>Project: https://egovisiongroup.github.io/Exo2Ego.github.io/</p></details> |
| **[Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment](http://arxiv.org/abs/2503.09081v1)** | 2025-03-12 |  |
| **[HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data](http://arxiv.org/abs/2412.17574v2)** | 2025-03-12 | <details><summary>22 pa...</summary><p>22 pages, 23 figures, 7 tables</p></details> |
| **[Measure Twice, Cut Once: Grasping Video Structures and Event Semantics with LLMs for Video Temporal Localization](http://arxiv.org/abs/2503.09027v1)** | 2025-03-12 |  |
| **[QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension](http://arxiv.org/abs/2503.08689v1)** | 2025-03-11 | <details><summary>Proje...</summary><p>Project page: https://github.com/MAC-AutoML/QuoTA</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1](http://arxiv.org/abs/2503.10635v1)** | 2025-03-13 | <details><summary>Code ...</summary><p>Code at: https://github.com/VILA-Lab/M-Attack</p></details> |
| **[HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](http://arxiv.org/abs/2503.10631v1)** | 2025-03-13 |  |
| **[CoSTA$\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing](http://arxiv.org/abs/2503.10613v1)** | 2025-03-13 |  |
| **[TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention](http://arxiv.org/abs/2503.10602v1)** | 2025-03-13 | <details><summary>15 pa...</summary><p>15 pages, 9 figures, the first two authors contributed equally</p></details> |
| **[GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding](http://arxiv.org/abs/2503.10596v1)** | 2025-03-13 | <details><summary>Work ...</summary><p>Work in progress. Code: https://github.com/hustvl/GroundingSuite</p></details> |
| **[VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search](http://arxiv.org/abs/2503.10582v1)** | 2025-03-13 | Technical Report |
| **[KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation](http://arxiv.org/abs/2503.10546v1)** | 2025-03-13 | <details><summary>Proje...</summary><p>Project website: http://kuda-dynamics.github.io</p></details> |
| **[World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning](http://arxiv.org/abs/2503.10480v1)** | 2025-03-13 |  |
| **[COMBO: Compositional World Models for Embodied Multi-Agent Cooperation](http://arxiv.org/abs/2404.10775v2)** | 2025-03-13 | <details><summary>Publi...</summary><p>Published at ICLR 2025. 24 pages. The first three authors contributed equally</p></details> |
| **[EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](http://arxiv.org/abs/2409.18042v3)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025. Project Page: https://emova-ollm.github.io/</p></details> |
| **[VisTai: Benchmarking Vision-Language Models for Traditional Chinese in Taiwan](http://arxiv.org/abs/2503.10427v1)** | 2025-03-13 |  |
| **[Reasoning to Attend: Try to Understand How <SEG> Token Works](http://arxiv.org/abs/2412.17741v6)** | 2025-03-13 | <details><summary>This ...</summary><p>This work has been accepted to CVPR 2025, please refer to https://github.com/rui-qian/READ</p></details> |
| **[Towards Fast, Memory-based and Data-Efficient Vision-Language Policy](http://arxiv.org/abs/2503.10322v1)** | 2025-03-13 | <details><summary>11 pa...</summary><p>11 pages, 7 figures, 6 tables</p></details> |
| **[Narrating the Video: Boosting Text-Video Retrieval via Comprehensive Utilization of Frame-Level Captions](http://arxiv.org/abs/2503.05186v3)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted at CVPR 2025</p></details> |
| **[SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical Intelligence](http://arxiv.org/abs/2503.10265v1)** | 2025-03-13 |  |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing](http://arxiv.org/abs/2503.10639v1)** | 2025-03-13 | <details><summary>Datas...</summary><p>Dataset and models are released in https://github.com/rongyaofang/GoT</p></details> |
| **[PiSA: A Self-Augmented Data Engine and Training Strategy for 3D Understanding with Large Models](http://arxiv.org/abs/2503.10529v1)** | 2025-03-13 | Technical Report |
| **[TokenCarve: Information-Preserving Visual Token Compression in Multimodal Large Language Models](http://arxiv.org/abs/2503.10501v1)** | 2025-03-13 |  |
| **[4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models](http://arxiv.org/abs/2503.10437v1)** | 2025-03-13 | <details><summary>CVPR ...</summary><p>CVPR 2025. Project Page: https://4d-langsplat.github.io</p></details> |
| **[CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance](http://arxiv.org/abs/2503.10391v1)** | 2025-03-13 |  |
| **[VisualPRM: An Effective Process Reward Model for Multimodal Reasoning](http://arxiv.org/abs/2503.10291v1)** | 2025-03-13 |  |
| **[FaVChat: Unlocking Fine-Grained Facial Video Understanding with Multimodal Large Language Models](http://arxiv.org/abs/2503.09158v2)** | 2025-03-13 |  |
| **[LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents](http://arxiv.org/abs/2503.10200v1)** | 2025-03-13 |  |
| **[Hybrid Agents for Image Restoration](http://arxiv.org/abs/2503.10120v1)** | 2025-03-13 |  |
| **[DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation](http://arxiv.org/abs/2412.07589v2)** | 2025-03-13 | <details><summary>[CVPR...</summary><p>[CVPR 2025] The project page is https://jianzongwu.github.io/projects/diffsensei/</p></details> |
| **[Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training](http://arxiv.org/abs/2410.08202v3)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[Information Density Principle for MLLM Benchmarks](http://arxiv.org/abs/2503.10079v1)** | 2025-03-13 |  |
| **[How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game](http://arxiv.org/abs/2503.10042v1)** | 2025-03-13 |  |
| **[MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](http://arxiv.org/abs/2502.17599v2)** | 2025-03-13 | NAACL 2025 Main |
| **[UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?](http://arxiv.org/abs/2503.09949v1)** | 2025-03-13 |  |

