---
title: Latest 15 Papers - March 06, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Anomaly detection in non-stationary videos using time-recursive differencing network based prediction](http://arxiv.org/abs/2503.02234v1)** | 2025-03-04 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos](http://arxiv.org/abs/2503.00670v1)** | 2025-03-02 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](http://arxiv.org/abs/2410.01506v4)** | 2025-02-28 | <details><summary>Accep...</summary><p>Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM](http://arxiv.org/abs/2502.18863v1)** | 2025-02-26 |  |
| **[An Attribute-based Method for Video Anomaly Detection](http://arxiv.org/abs/2212.00789v2)** | 2025-01-26 | <details><summary>TMLR ...</summary><p>TMLR 2025. Our code is available at https://github.com/talreiss/Accurate-Interpretable-VAD</p></details> |
| **[Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2406.02831v2)** | 2025-01-18 |  |
| **[ComplexVAD: Detecting Interaction Anomalies in Video](http://arxiv.org/abs/2501.09733v1)** | 2025-01-16 | <details><summary>16 pa...</summary><p>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</p></details> |
| **[Detecting Contextual Anomalies by Discovering Consistent Spatial Regions](http://arxiv.org/abs/2501.08470v1)** | 2025-01-14 |  |
| **[Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive Knowledge Graph Learning](http://arxiv.org/abs/2411.09072v2)** | 2025-01-14 | <details><summary>Accep...</summary><p>Accepted to DATE 2025</p></details> |
| **[Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2412.20455v1)** | 2024-12-29 | <details><summary>Accep...</summary><p>Accepted to CVPR'24 MULA Workshop</p></details> |
| **[Exploring the Magnitude-Shape Plot Framework for Anomaly Detection in Crowded Video Scenes](http://arxiv.org/abs/2412.20363v1)** | 2024-12-29 | <details><summary>21 pa...</summary><p>21 pages, 4 figures, 10 tables</p></details> |
| **[Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems](http://arxiv.org/abs/2412.20201v1)** | 2024-12-28 | <details><summary>IEEE ...</summary><p>IEEE TETC-CS (Under review)</p></details> |
| **[STNMamba: Mamba-based Spatial-Temporal Normality Learning for Video Anomaly Detection](http://arxiv.org/abs/2412.20084v1)** | 2024-12-28 |  |
| **[Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight](http://arxiv.org/abs/2412.18298v1)** | 2024-12-24 | Research report |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v1)** | 2024-12-23 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AVD2: Accident Video Diffusion for Accident Video Description](http://arxiv.org/abs/2502.14801v3)** | 2025-03-04 | <details><summary>ICRA ...</summary><p>ICRA 2025, Project Page: https://an-answer-tree.github.io/</p></details> |
| **[Frame-Voyager: Learning to Query Frames for Video Large Language Models](http://arxiv.org/abs/2410.03226v3)** | 2025-03-04 | <details><summary>ICLR ...</summary><p>ICLR 2025, Camera-ready Version</p></details> |
| **[TRACE: Temporal Grounding Video LLM via Causal Event Modeling](http://arxiv.org/abs/2410.05643v3)** | 2025-03-03 | ICLR 2025 |
| **[Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning](http://arxiv.org/abs/2503.00986v1)** | 2025-03-02 | <details><summary>Accep...</summary><p>Accepted as ICLR 2025 conference paper</p></details> |
| **[Q-Bench-Video: Benchmarking the Video Quality Understanding of LMMs](http://arxiv.org/abs/2409.20063v2)** | 2025-03-02 |  |
| **[AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](http://arxiv.org/abs/2410.03051v3)** | 2025-03-02 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025. Code, docs, weight, benchmark and training data are all avaliable at https://rese1f.github.io/aurora-web/</p></details> |
| **[PreMind: Multi-Agent Video Understanding for Advanced Indexing of Presentation-style Videos](http://arxiv.org/abs/2503.00162v1)** | 2025-02-28 |  |
| **[Adaptive Keyframe Sampling for Long Video Understanding](http://arxiv.org/abs/2502.21271v1)** | 2025-02-28 | CVPR2025 |
| **[HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](http://arxiv.org/abs/2502.20811v1)** | 2025-02-28 |  |
| **[OpenTAD: A Unified Framework and Comprehensive Study of Temporal Action Detection](http://arxiv.org/abs/2502.20361v1)** | 2025-02-27 |  |
| **[MaxInfo: A Training-Free Key-Frame Selection Method Using Maximum Volume for Enhanced Video Understanding](http://arxiv.org/abs/2502.03183v2)** | 2025-02-27 |  |
| **[Ego-VPA: Egocentric Video Understanding with Parameter-efficient Adaptation](http://arxiv.org/abs/2407.19520v2)** | 2025-02-27 | <details><summary>Accep...</summary><p>Accepted to WACV 2025</p></details> |
| **[MAMBA4D: Efficient Long-Sequence Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models](http://arxiv.org/abs/2405.14338v3)** | 2025-02-27 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025. The first two authors contribute equally</p></details> |
| **[A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems](http://arxiv.org/abs/2502.06581v3)** | 2025-02-27 |  |
| **[M-LLM Based Video Frame Selection for Efficient Video Understanding](http://arxiv.org/abs/2502.19680v1)** | 2025-02-27 |  |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge](http://arxiv.org/abs/2411.12915v3)** | 2025-03-04 |  |
| **[FairSense-AI: Responsible AI Meets Sustainability](http://arxiv.org/abs/2503.02865v1)** | 2025-03-04 |  |
| **[Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas](http://arxiv.org/abs/2503.01773v2)** | 2025-03-04 |  |
| **["What If Smart Homes Could See Our Homes?": Exploring DIY Smart Home Building Experiences with VLM-Based Camera Sensors](http://arxiv.org/abs/2503.02816v1)** | 2025-03-04 |  |
| **[Bridging VLM and KMP: Enabling Fine-grained robotic manipulation via Semantic Keypoints Representation](http://arxiv.org/abs/2503.02748v1)** | 2025-03-04 |  |
| **[ImpedanceGPT: VLM-driven Impedance Control of Swarm of Mini-drones for Intelligent Navigation in Dynamic Environment](http://arxiv.org/abs/2503.02723v1)** | 2025-03-04 | <details><summary>Submi...</summary><p>Submitted to IROS 2025</p></details> |
| **[WalkVLM:Aid Visually Impaired People Walking by Vision Language Model](http://arxiv.org/abs/2412.20903v4)** | 2025-03-04 |  |
| **[MM-OR: A Large Multimodal Operating Room Dataset for Semantic Understanding of High-Intensity Surgical Environments](http://arxiv.org/abs/2503.02579v1)** | 2025-03-04 |  |
| **[InstaFace: Identity-Preserving Facial Editing with Single Image Inference](http://arxiv.org/abs/2502.20577v2)** | 2025-03-04 |  |
| **[Vision-Language Model IP Protection via Prompt-based Learning](http://arxiv.org/abs/2503.02393v1)** | 2025-03-04 |  |
| **[A Survey on Vision-Language-Action Models for Embodied AI](http://arxiv.org/abs/2405.14093v4)** | 2025-03-04 | <details><summary>Proje...</summary><p>Project page: https://github.com/yueen-ma/Awesome-VLA</p></details> |
| **[DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models](http://arxiv.org/abs/2411.09022v2)** | 2025-03-04 | <details><summary>The w...</summary><p>The work was first submitted to an IEEE conference on September 15, 2024</p></details> |
| **[Are Large Vision Language Models Good Game Players?](http://arxiv.org/abs/2503.02358v1)** | 2025-03-04 | ICLR2025 |
| **[BiasICL: In-Context Learning and Demographic Biases of Vision Language Models](http://arxiv.org/abs/2503.02334v1)** | 2025-03-04 |  |
| **[WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation](http://arxiv.org/abs/2503.02247v1)** | 2025-03-04 | 8 pages, 5 figures |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning](http://arxiv.org/abs/2503.00043v2)** | 2025-03-04 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025. Code and data: https://github.com/nlylmz/Voila</p></details> |
| **[Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs](http://arxiv.org/abs/2503.02597v1)** | 2025-03-04 | Preprint |
| **[MciteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs](http://arxiv.org/abs/2503.02589v1)** | 2025-03-04 |  |
| **[Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](http://arxiv.org/abs/2502.16033v2)** | 2025-03-04 |  |
| **[SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model](http://arxiv.org/abs/2412.01550v2)** | 2025-03-04 |  |
| **[Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](http://arxiv.org/abs/2410.05243v2)** | 2025-03-03 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025 (Oral)</p></details> |
| **[FoodMLLM-JP: Leveraging Multimodal Large Language Models for Japanese Recipe Generation](http://arxiv.org/abs/2409.18459v2)** | 2025-03-03 | <details><summary>15 pa...</summary><p>15 pages, 5 figures. We found errors in the calculation of evaluation metrics, which were corrected in this version with $\color{blue}{\text{modifications highlighted in blue}}$. Please also see the Appendix</p></details> |
| **[Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention](http://arxiv.org/abs/2502.19877v2)** | 2025-03-03 | <details><summary>Accep...</summary><p>Accepted to ACM CHI Conference on Human Factors in Computing Systems Late Breaking Work (CHI'25 LBW)</p></details> |
| **[RemiHaven: Integrating "In-Town" and "Out-of-Town" Peers to Provide Personalized Reminiscence Support for Older Drifters](http://arxiv.org/abs/2503.01358v1)** | 2025-03-03 |  |
| **[Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG](http://arxiv.org/abs/2503.01222v1)** | 2025-03-03 |  |
| **[MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts](http://arxiv.org/abs/2502.20808v2)** | 2025-03-03 | 47 pages |
| **[Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders](http://arxiv.org/abs/2408.15998v2)** | 2025-03-02 | <details><summary>Githu...</summary><p>Github: https://github.com/NVlabs/Eagle, HuggingFace: https://huggingface.co/NVEagle</p></details> |
| **[SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding](http://arxiv.org/abs/2411.01106v2)** | 2025-03-02 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[From Screens to Scenes: A Survey of Embodied AI in Healthcare](http://arxiv.org/abs/2501.07468v3)** | 2025-03-02 | <details><summary>56 pa...</summary><p>56 pages, 11 figures, manuscript accepted by Information Fusion</p></details> |
| **[FunBench: Benchmarking Fundus Reading Skills of MLLMs](http://arxiv.org/abs/2503.00901v1)** | 2025-03-02 | 7 pages |

