---
title: Latest 15 Papers - March 25, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |
| **[Language-guided Open-world Video Anomaly Detection](http://arxiv.org/abs/2503.13160v1)** | 2025-03-17 |  |
| **[UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks](http://arxiv.org/abs/2503.12905v1)** | 2025-03-17 | <details><summary>Accep...</summary><p>Accepted by AAAI 2025</p></details> |
| **[Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos](http://arxiv.org/abs/2503.07799v1)** | 2025-03-10 |  |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Structured Keywords](http://arxiv.org/abs/2503.10653v1)** | 2025-03-07 |  |
| **[AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](http://arxiv.org/abs/2503.04504v1)** | 2025-03-06 |  |
| **[Anomaly detection in non-stationary videos using time-recursive differencing network based prediction](http://arxiv.org/abs/2503.02234v1)** | 2025-03-04 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos](http://arxiv.org/abs/2503.00670v1)** | 2025-03-02 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](http://arxiv.org/abs/2410.01506v4)** | 2025-02-28 | <details><summary>Accep...</summary><p>Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM](http://arxiv.org/abs/2502.18863v1)** | 2025-02-26 |  |
| **[An Attribute-based Method for Video Anomaly Detection](http://arxiv.org/abs/2212.00789v2)** | 2025-01-26 | <details><summary>TMLR ...</summary><p>TMLR 2025. Our code is available at https://github.com/talreiss/Accurate-Interpretable-VAD</p></details> |
| **[Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2406.02831v2)** | 2025-01-18 |  |
| **[ComplexVAD: Detecting Interaction Anomalies in Video](http://arxiv.org/abs/2501.09733v1)** | 2025-01-16 | <details><summary>16 pa...</summary><p>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</p></details> |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[PVChat: Personalized Video Chat with One-Shot Learning](http://arxiv.org/abs/2503.17069v1)** | 2025-03-21 |  |
| **[Temporal Action Detection Model Compression by Progressive Block Drop](http://arxiv.org/abs/2503.16916v1)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **[XAttention: Block Sparse Attention with Antidiagonal Scoring](http://arxiv.org/abs/2503.16428v1)** | 2025-03-20 | <details><summary>The f...</summary><p>The first two authors contributed equally to this work</p></details> |
| **[Wolf: Dense Video Captioning with a World Summarization Framework](http://arxiv.org/abs/2407.18908v2)** | 2025-03-20 |  |
| **[LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos](http://arxiv.org/abs/2411.19772v3)** | 2025-03-20 | Accepted by CVPR2025 |
| **[Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models](http://arxiv.org/abs/2503.16036v1)** | 2025-03-20 | Accepted to CVPR2025 |
| **[Agentic Keyframe Search for Video Question Answering](http://arxiv.org/abs/2503.16032v1)** | 2025-03-20 |  |
| **[STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding](http://arxiv.org/abs/2503.15973v1)** | 2025-03-20 |  |
| **[DocVideoQA: Towards Comprehensive Understanding of Document-Centric Videos through Question Answering](http://arxiv.org/abs/2503.15887v1)** | 2025-03-20 |  |
| **[MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations](http://arxiv.org/abs/2503.15871v1)** | 2025-03-20 | <details><summary>Accep...</summary><p>Accepted for CVPR 2025</p></details> |
| **[What can Off-the-Shelves Large Multi-Modal Models do for Dynamic Scene Graph Generation?](http://arxiv.org/abs/2503.15846v1)** | 2025-03-20 |  |
| **[ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models](http://arxiv.org/abs/2411.10867v2)** | 2025-03-19 |  |
| **[FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding](http://arxiv.org/abs/2503.14935v1)** | 2025-03-19 | <details><summary>FAVOR...</summary><p>FAVOR-Bench project page: https://favor-bench.github.io/</p></details> |
| **[Impossible Videos](http://arxiv.org/abs/2503.14378v1)** | 2025-03-18 | 26 pages |
| **[CaReBench: A Fine-Grained Benchmark for Video Captioning and Retrieval](http://arxiv.org/abs/2501.00513v2)** | 2025-03-18 |  |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement](http://arxiv.org/abs/2503.17352v1)** | 2025-03-21 | <details><summary>23 pa...</summary><p>23 pages, 11 figures, 8 tables</p></details> |
| **[Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models](http://arxiv.org/abs/2503.17349v1)** | 2025-03-21 |  |
| **[TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention](http://arxiv.org/abs/2503.10602v2)** | 2025-03-21 | <details><summary>15 pa...</summary><p>15 pages, 9 figures, the first two authors contributed equally</p></details> |
| **[Slide-Level Prompt Learning with Vision Language Models for Few-Shot Multiple Instance Learning in Histopathology](http://arxiv.org/abs/2503.17238v1)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted to ISBI 2025</p></details> |
| **[Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection](http://arxiv.org/abs/2412.04455v3)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025. Project page: https://zhoues.github.io/Code-as-Monitor/</p></details> |
| **[VASparse: Towards Efficient Visual Hallucination Mitigation via Visual-Aware Token Sparsification](http://arxiv.org/abs/2501.06553v2)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models](http://arxiv.org/abs/2503.17142v1)** | 2025-03-21 | <details><summary>Camer...</summary><p>Camera-ready version for CVPR 2025 (with Supp.Mat.)</p></details> |
| **[T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting](http://arxiv.org/abs/2502.20625v2)** | 2025-03-21 | Accepted by CVPR2025 |
| **[Beyond Accuracy: What Matters in Designing Well-Behaved Models?](http://arxiv.org/abs/2503.17110v1)** | 2025-03-21 | <details><summary>Code:...</summary><p>Code: https://github.com/visinf/beyond-accuracy</p></details> |
| **[Morphing Tokens Draw Strong Masked Image Models](http://arxiv.org/abs/2401.00254v4)** | 2025-03-21 | <details><summary>24 pa...</summary><p>24 pages, 16 tables, 8 figures. To be presented at ICLR'25</p></details> |
| **[Enhanced Continual Learning of Vision-Language Models with Model Fusion](http://arxiv.org/abs/2503.10705v2)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025 workshop</p></details> |
| **[PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition](http://arxiv.org/abs/2503.16945v1)** | 2025-03-21 |  |
| **[When Lighting Deceives: Exposing Vision-Language Models' Illumination Vulnerability Through Illumination Transformation Attack](http://arxiv.org/abs/2503.06903v2)** | 2025-03-21 |  |
| **[A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees](http://arxiv.org/abs/2503.15202v2)** | 2025-03-21 |  |
| **[Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks](http://arxiv.org/abs/2503.16930v1)** | 2025-03-21 | CVPR 2025 |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[GiVE: Guiding Visual Encoder to Perceive Overlooked Information](http://arxiv.org/abs/2410.20109v2)** | 2025-03-21 | <details><summary>This ...</summary><p>This paper was accepted by ICME 2025</p></details> |
| **[EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering](http://arxiv.org/abs/2502.07411v2)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?](http://arxiv.org/abs/2503.09949v2)** | 2025-03-21 |  |
| **[Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification](http://arxiv.org/abs/2412.00876v4)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025. Code is available at https://github.com/Osilly/dynamic_llava</p></details> |
| **[Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention](http://arxiv.org/abs/2502.19877v3)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted at ACM 2025 Conference on Human Factors in Computing Systems Late Breaking Work (CHI'25 LBW)</p></details> |
| **[SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models](http://arxiv.org/abs/2402.05935v3)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted by ICML 2024. Code and models are released at https://github.com/Alpha-VLLM/LLaMA2-Accessory</p></details> |
| **[Lie Detector: Unified Backdoor Detection via Cross-Examination Framework](http://arxiv.org/abs/2503.16872v1)** | 2025-03-21 |  |
| **[LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models](http://arxiv.org/abs/2503.16843v1)** | 2025-03-21 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model](http://arxiv.org/abs/2412.01550v3)** | 2025-03-21 |  |
| **[When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts](http://arxiv.org/abs/2503.16826v1)** | 2025-03-21 | 12 pages |
| **[Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs](http://arxiv.org/abs/2503.12303v4)** | 2025-03-21 | <details><summary>38 pa...</summary><p>38 pages. Preprint, work in progress</p></details> |
| **[Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](http://arxiv.org/abs/2411.02937v4)** | 2025-03-21 |  |
| **[CUE-M: Contextual Understanding and Enhanced Search with Multimodal Large Language Model](http://arxiv.org/abs/2411.12287v3)** | 2025-03-21 | <details><summary>Prepr...</summary><p>Preprint. Under review</p></details> |
| **[Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models](http://arxiv.org/abs/2503.16734v1)** | 2025-03-20 |  |
| **[OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence](http://arxiv.org/abs/2503.16326v1)** | 2025-03-20 | <details><summary>15 pa...</summary><p>15 pages, Under review</p></details> |

