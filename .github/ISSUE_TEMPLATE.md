---
title: Latest 15 Papers - February 19, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](http://arxiv.org/abs/2410.01506v3)** | 2025-02-05 | <details><summary>Accep...</summary><p>Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[An Attribute-based Method for Video Anomaly Detection](http://arxiv.org/abs/2212.00789v2)** | 2025-01-26 | <details><summary>TMLR ...</summary><p>TMLR 2025. Our code is available at https://github.com/talreiss/Accurate-Interpretable-VAD</p></details> |
| **[Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2406.02831v2)** | 2025-01-18 |  |
| **[ComplexVAD: Detecting Interaction Anomalies in Video](http://arxiv.org/abs/2501.09733v1)** | 2025-01-16 | <details><summary>16 pa...</summary><p>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</p></details> |
| **[Detecting Contextual Anomalies by Discovering Consistent Spatial Regions](http://arxiv.org/abs/2501.08470v1)** | 2025-01-14 |  |
| **[Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive Knowledge Graph Learning](http://arxiv.org/abs/2411.09072v2)** | 2025-01-14 | <details><summary>Accep...</summary><p>Accepted to DATE 2025</p></details> |
| **[Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2412.20455v1)** | 2024-12-29 | <details><summary>Accep...</summary><p>Accepted to CVPR'24 MULA Workshop</p></details> |
| **[Exploring the Magnitude-Shape Plot Framework for Anomaly Detection in Crowded Video Scenes](http://arxiv.org/abs/2412.20363v1)** | 2024-12-29 | <details><summary>21 pa...</summary><p>21 pages, 4 figures, 10 tables</p></details> |
| **[Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems](http://arxiv.org/abs/2412.20201v1)** | 2024-12-28 | <details><summary>IEEE ...</summary><p>IEEE TETC-CS (Under review)</p></details> |
| **[STNMamba: Mamba-based Spatial-Temporal Normality Learning for Video Anomaly Detection](http://arxiv.org/abs/2412.20084v1)** | 2024-12-28 |  |
| **[Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight](http://arxiv.org/abs/2412.18298v1)** | 2024-12-24 | Research report |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v1)** | 2024-12-23 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Motion and Appearance Guided Patch Diffusion Model](http://arxiv.org/abs/2412.09026v1)** | 2024-12-12 | Accept by AAAI2025 |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v1)** | 2024-12-04 |  |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v1)** | 2024-12-02 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Understanding Long Videos with Multimodal Language Models](http://arxiv.org/abs/2403.16998v3)** | 2025-02-17 | <details><summary>Code ...</summary><p>Code available at https://github.com/kahnchana/mvu</p></details> |
| **[video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model](http://arxiv.org/abs/2502.11775v1)** | 2025-02-17 |  |
| **[VRoPE: Rotary Position Embedding for Video Large Language Models](http://arxiv.org/abs/2502.11664v1)** | 2025-02-17 | 10 pages |
| **[iMOVE: Instance-Motion-Aware Video Understanding](http://arxiv.org/abs/2502.11594v1)** | 2025-02-17 |  |
| **[Do Language Models Understand Time?](http://arxiv.org/abs/2412.13845v2)** | 2025-02-16 | <details><summary>Accep...</summary><p>Accepted for publication in the Companion Proceedings of the ACM Web Conference (WWW Companion 2025)</p></details> |
| **[SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](http://arxiv.org/abs/2502.10810v1)** | 2025-02-15 | <details><summary>ICLR ...</summary><p>ICLR 2025 Accept (Spotlight)</p></details> |
| **[Semantics-aware Test-time Adaptation for 3D Human Pose Estimation](http://arxiv.org/abs/2502.10724v1)** | 2025-02-15 | 10 pages, 4 figures |
| **[VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks](http://arxiv.org/abs/2410.19100v3)** | 2025-02-15 |  |
| **[Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering](http://arxiv.org/abs/2502.09573v2)** | 2025-02-14 |  |
| **[TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning](http://arxiv.org/abs/2410.19702v2)** | 2025-02-12 | Accepted by ICLR2025 |
| **[A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems](http://arxiv.org/abs/2502.06581v2)** | 2025-02-12 |  |
| **[CoS: Chain-of-Shot Prompting for Long Video Understanding](http://arxiv.org/abs/2502.06428v2)** | 2025-02-11 | <details><summary>A tra...</summary><p>A training-free test-time optimisation approach for long video understanding</p></details> |
| **[Enhancing Video Understanding: Deep Neural Networks for Spatiotemporal Analysis](http://arxiv.org/abs/2502.07277v1)** | 2025-02-11 | 29 pages, 25 figures |
| **[A Survey on Mamba Architecture for Vision Applications](http://arxiv.org/abs/2502.07161v1)** | 2025-02-11 |  |
| **[BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes](http://arxiv.org/abs/2404.03161v2)** | 2025-02-11 | 6 pages |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Descriminative-Generative Custom Tokens for Vision-Language Models](http://arxiv.org/abs/2502.12095v1)** | 2025-02-17 |  |
| **[VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](http://arxiv.org/abs/2502.12084v1)** | 2025-02-17 | <details><summary>Proje...</summary><p>Project Page: https://vlm2-bench.github.io/</p></details> |
| **[Understanding Figurative Meaning through Explainable Visual Entailment](http://arxiv.org/abs/2405.01474v3)** | 2025-02-17 | <details><summary>NAACL...</summary><p>NAACL 2025 Main Conference</p></details> |
| **[HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation](http://arxiv.org/abs/2502.09838v2)** | 2025-02-17 | <details><summary>Comme...</summary><p>Comments: added project page</p></details> |
| **[Learning Generalizable Prompt for CLIP with Class Similarity Knowledge](http://arxiv.org/abs/2502.11969v1)** | 2025-02-17 |  |
| **[Generative Semantic Communication via Textual Prompts: Latency Performance Tradeoffs](http://arxiv.org/abs/2409.09715v2)** | 2025-02-17 |  |
| **[From Open-Vocabulary to Vocabulary-Free Semantic Segmentation](http://arxiv.org/abs/2502.11891v1)** | 2025-02-17 | <details><summary>Submi...</summary><p>Submitted to: Pattern Recognition Letters, Klara Reichard and Giulia Rizzoli equally contributed to this work</p></details> |
| **[Better Language Models Exhibit Higher Visual Alignment](http://arxiv.org/abs/2410.07173v2)** | 2025-02-17 |  |
| **[ILIAS: Instance-Level Image retrieval At Scale](http://arxiv.org/abs/2502.11748v1)** | 2025-02-17 |  |
| **[Multi-Modal Retrieval Augmentation for Open-Ended and Knowledge-Intensive Video Question Answering](http://arxiv.org/abs/2502.11747v1)** | 2025-02-17 |  |
| **[Scalable Vision Language Model Training via High Quality Data Curation](http://arxiv.org/abs/2501.05952v2)** | 2025-02-17 |  |
| **["See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models](http://arxiv.org/abs/2502.11718v1)** | 2025-02-17 | 24 pages, 21 figures |
| **[Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision-Language Models](http://arxiv.org/abs/2411.03888v2)** | 2025-02-17 | <details><summary>Accep...</summary><p>Accepted to NAACL 2025 Main (Camera-Ready Version)</p></details> |
| **[Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation](http://arxiv.org/abs/2312.10807v4)** | 2025-02-17 | <details><summary>37 pa...</summary><p>37 pages, 15 figures, 4 tables, 354 citations</p></details> |
| **[MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease Progression](http://arxiv.org/abs/2502.11651v1)** | 2025-02-17 |  |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation](http://arxiv.org/abs/2502.12148v1)** | 2025-02-17 | <details><summary>Code:...</summary><p>Code: https://github.com/Gen-Verse/HermesFlow</p></details> |
| **[PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection](http://arxiv.org/abs/2502.12119v1)** | 2025-02-17 |  |
| **[Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination](http://arxiv.org/abs/2411.03823v2)** | 2025-02-17 | <details><summary>Code ...</summary><p>Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect</p></details> |
| **[Token Communications: A Unified Framework for Cross-modal Context-aware Semantic Communications](http://arxiv.org/abs/2502.12096v1)** | 2025-02-17 |  |
| **[GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs](http://arxiv.org/abs/2502.11925v1)** | 2025-02-17 |  |
| **[Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework](http://arxiv.org/abs/2412.19684v2)** | 2025-02-17 |  |
| **[EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models](http://arxiv.org/abs/2502.11916v1)** | 2025-02-17 | <details><summary>JS an...</summary><p>JS and YY are co-first authors. XH is the corresponding author</p></details> |
| **[Bridging Compressed Image Latents and Multimodal Large Language Models](http://arxiv.org/abs/2407.19651v2)** | 2025-02-17 | <details><summary>Accep...</summary><p>Accepted by ICLR 2025</p></details> |
| **[MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation](http://arxiv.org/abs/2502.11903v1)** | 2025-02-17 |  |
| **[Intuitive physics understanding emerges from self-supervised pretraining on natural videos](http://arxiv.org/abs/2502.11831v1)** | 2025-02-17 | <details><summary>24 pa...</summary><p>24 pages,14 figures, 5 tables</p></details> |
| **[Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities](http://arxiv.org/abs/2502.11829v1)** | 2025-02-17 | 15 pages |
| **[Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning](http://arxiv.org/abs/2502.11751v1)** | 2025-02-17 | <details><summary>Accep...</summary><p>Accepted to ICASSP 2025</p></details> |
| **[InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning](http://arxiv.org/abs/2502.11573v1)** | 2025-02-17 |  |
| **[Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?](http://arxiv.org/abs/2502.11501v1)** | 2025-02-17 | 12 pages, 3 figures |
| **[Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More](http://arxiv.org/abs/2502.11494v1)** | 2025-02-17 | 15 pages, 8 figures |

