---
title: Latest 15 Papers - April 24, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches](http://arxiv.org/abs/2504.14753v1)** | 2025-04-20 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Image Processing (TIP)</p></details> |
| **[EventVAD: Training-Free Event-Aware Video Anomaly Detection](http://arxiv.org/abs/2504.13092v1)** | 2025-04-17 |  |
| **[SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model](http://arxiv.org/abs/2504.10320v1)** | 2025-04-14 |  |
| **[AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection](http://arxiv.org/abs/2504.04495v1)** | 2025-04-06 | <details><summary>11 pa...</summary><p>11 pages, 4 figures, 6 tables</p></details> |
| **[From Explicit Rules to Implicit Reasoning in Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2410.21991v6)** | 2025-04-06 | <details><summary>This ...</summary><p>This manuscript has been submitted to IEEE Transactions on Circuits and Systems for Video Technology and is under consideration for publication, with potential copyright transfer in the future</p></details> |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v4)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to ACM Computing Surveys. For more information and supplementary material, please visit https://github.com/fdjingliu/NSVAD</p></details> |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v3)** | 2025-03-31 | <details><summary>Accep...</summary><p>Accepted in CVPR 2025</p></details> |
| **[AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis](http://arxiv.org/abs/2503.21904v1)** | 2025-03-27 | 13 pages |
| **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)** | 2025-03-27 | <details><summary>Accpe...</summary><p>Accpeted by ICME 2025</p></details> |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)** | 2025-03-27 |  |
| **[Video Anomaly Detection with Contours -- A Study](http://arxiv.org/abs/2503.19588v1)** | 2025-03-25 |  |
| **[CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2503.18808v1)** | 2025-03-24 | <details><summary>Accep...</summary><p>Accepted for publication by IEEE Transactions on Image Processing</p></details> |
| **[Anomize: Better Open Vocabulary Video Anomaly Detection](http://arxiv.org/abs/2503.18094v1)** | 2025-03-23 |  |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MR. Video: "MapReduce" is the Principle for Long Video Understanding](http://arxiv.org/abs/2504.16082v1)** | 2025-04-22 | Preprint |
| **[ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](http://arxiv.org/abs/2504.15921v1)** | 2025-04-22 |  |
| **[Vidi: Large Multimodal Models for Video Understanding and Editing](http://arxiv.org/abs/2504.15681v1)** | 2025-04-22 |  |
| **[IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](http://arxiv.org/abs/2504.15415v1)** | 2025-04-21 |  |
| **[Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](http://arxiv.org/abs/2504.15271v1)** | 2025-04-21 |  |
| **[An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](http://arxiv.org/abs/2504.15270v1)** | 2025-04-21 |  |
| **[Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](http://arxiv.org/abs/2504.02438v3)** | 2025-04-21 |  |
| **[PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild](http://arxiv.org/abs/2504.11326v2)** | 2025-04-21 | <details><summary>Works...</summary><p>Workshop Page: https://pvuw.github.io/. arXiv admin note: text overlap with arXiv:2504.00476, arXiv:2504.05178</p></details> |
| **[OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](http://arxiv.org/abs/2504.14692v1)** | 2025-04-20 |  |
| **[Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection](http://arxiv.org/abs/2504.14553v1)** | 2025-04-20 |  |
| **[Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](http://arxiv.org/abs/2504.14526v1)** | 2025-04-20 |  |
| **[ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](http://arxiv.org/abs/2504.14432v1)** | 2025-04-20 |  |
| **[How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?](http://arxiv.org/abs/2504.14391v1)** | 2025-04-19 |  |
| **[3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](http://arxiv.org/abs/2412.07825v2)** | 2025-04-19 | <details><summary>Proje...</summary><p>Project page: https://3dsrbench.github.io</p></details> |
| **[Perception Encoder: The best visual embeddings are not at the output of the network](http://arxiv.org/abs/2504.13181v1)** | 2025-04-17 | Initial Submission |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](http://arxiv.org/abs/2504.16083v1)** | 2025-04-22 |  |
| **[MR. Video: "MapReduce" is the Principle for Long Video Understanding](http://arxiv.org/abs/2504.16082v1)** | 2025-04-22 | Preprint |
| **[Describe Anything: Detailed Localized Image and Video Captioning](http://arxiv.org/abs/2504.16072v1)** | 2025-04-22 | <details><summary>Proje...</summary><p>Project page: https://describe-anything.github.io/</p></details> |
| **[Vision language models are unreliable at trivial spatial cognition](http://arxiv.org/abs/2504.16061v1)** | 2025-04-22 |  |
| **[Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](http://arxiv.org/abs/2504.16060v1)** | 2025-04-22 | <details><summary>Homep...</summary><p>Homepage: https://vlm-reg.github.io/</p></details> |
| **[Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis](http://arxiv.org/abs/2504.16047v1)** | 2025-04-22 |  |
| **[LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](http://arxiv.org/abs/2504.16030v1)** | 2025-04-22 | <details><summary>CVPR ...</summary><p>CVPR 2025. If any references are missing, please contact joyachen@u.nus.edu</p></details> |
| **[Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models](http://arxiv.org/abs/2504.15929v1)** | 2025-04-22 | <details><summary>18 pa...</summary><p>18 pages, 7 figures, 6 tables</p></details> |
| **[Enhancing Features in Long-tailed Data Using Large Vision Model](http://arxiv.org/abs/2504.10852v2)** | 2025-04-22 |  |
| **[Red Team Diffuser: Exposing Toxic Continuation Vulnerabilities in Vision-Language Models via Reinforcement Learning](http://arxiv.org/abs/2503.06223v2)** | 2025-04-22 |  |
| **[NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](http://arxiv.org/abs/2410.14669v3)** | 2025-04-22 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 24; We open-source our dataset at: https://huggingface.co/datasets/BaiqiL/NaturalBench ; Project page at: https://linzhiqiu.github.io/papers/naturalbench/</p></details> |
| **[Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](http://arxiv.org/abs/2504.13945v2)** | 2025-04-22 | <details><summary>12 pa...</summary><p>12 pages, 5 figures, 5 Tables</p></details> |
| **[CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](http://arxiv.org/abs/2504.15485v1)** | 2025-04-21 | <details><summary>Code ...</summary><p>Code and data: https://github.com/atinpothiraj/CAPTURe</p></details> |
| **[Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](http://arxiv.org/abs/2504.15271v1)** | 2025-04-21 |  |
| **[Analysing the Robustness of Vision-Language-Models to Common Corruptions](http://arxiv.org/abs/2504.13690v2)** | 2025-04-21 | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2304.10592, arXiv:2301.12597 by other authors</p></details> |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger](http://arxiv.org/abs/2408.09093v3)** | 2025-04-22 |  |
| **[TextSquare: Scaling up Text-Centric Visual Instruction Tuning](http://arxiv.org/abs/2404.12803v2)** | 2025-04-22 |  |
| **[FaceInsight: A Multimodal Large Language Model for Face Perception](http://arxiv.org/abs/2504.15624v1)** | 2025-04-22 |  |
| **[AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization](http://arxiv.org/abs/2504.15619v1)** | 2025-04-22 |  |
| **[Manipulating Multimodal Agents via Cross-Modal Prompt Injection](http://arxiv.org/abs/2504.14348v2)** | 2025-04-22 | 17 pages, 5 figures |
| **[See or Recall: A Sanity Check for the Role of Vision in Solving Visualization Question Answer Tasks with Multimodal LLMs](http://arxiv.org/abs/2504.09809v2)** | 2025-04-21 |  |
| **[IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](http://arxiv.org/abs/2504.15415v1)** | 2025-04-21 |  |
| **[VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](http://arxiv.org/abs/2504.15279v1)** | 2025-04-21 | <details><summary>Code,...</summary><p>Code, data, and baselines are available at https://visulogic-benchmark.github.io/VisuLogic</p></details> |
| **[STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](http://arxiv.org/abs/2503.23765v3)** | 2025-04-21 |  |
| **[A Call for New Recipes to Enhance Spatial Reasoning in MLLMs](http://arxiv.org/abs/2504.15037v1)** | 2025-04-21 |  |
| **[GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation](http://arxiv.org/abs/2504.09587v2)** | 2025-04-21 |  |
| **[Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens](http://arxiv.org/abs/2504.14666v1)** | 2025-04-20 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025 (Oral)</p></details> |
| **[UFO2: The Desktop AgentOS](http://arxiv.org/abs/2504.14603v1)** | 2025-04-20 | <details><summary>The s...</summary><p>The source code of UFO2 is publicly available at https://github.com/microsoft/UFO/, with comprehensive documentation provided at https://microsoft.github.io/UFO/</p></details> |
| **[Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction](http://arxiv.org/abs/2504.14588v1)** | 2025-04-20 | Accepted by CVPR2025 |
| **[FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding](http://arxiv.org/abs/2504.09925v2)** | 2025-04-19 |  |

