---
title: Latest 15 Papers - November 06, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](http://arxiv.org/abs/2511.00962v1)** | 2025-11-02 | NeurIPS 2025 poster |
| **[TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](http://arxiv.org/abs/2511.00580v1)** | 2025-11-01 | 10 pages, 5 figures |
| **[Text-guided Fine-Grained Video Anomaly Detection](http://arxiv.org/abs/2511.00524v1)** | 2025-11-01 |  |
| **[Rethinking Metrics and Benchmarks of Video Anomaly Detection](http://arxiv.org/abs/2505.19022v2)** | 2025-10-31 |  |
| **[Language-guided Open-world Video Anomaly Detection under Weak Supervision](http://arxiv.org/abs/2503.13160v2)** | 2025-10-30 |  |
| **[VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree](http://arxiv.org/abs/2510.22693v2)** | 2025-10-28 | NeurIPS 2025 poster |
| **[PANDA: Towards Generalist Video Anomaly Detection via Agentic AI Engineer](http://arxiv.org/abs/2509.26386v2)** | 2025-10-28 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2025</p></details> |
| **[MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2410.05900v2)** | 2025-10-28 |  |
| **[FrameShield: Adversarially Robust Video Anomaly Detection](http://arxiv.org/abs/2510.21532v1)** | 2025-10-24 | 28 page, 5 figures |
| **[MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection](http://arxiv.org/abs/2510.21449v1)** | 2025-10-24 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025. The first two authors hold equal contributions</p></details> |
| **[GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection](http://arxiv.org/abs/2510.20268v1)** | 2025-10-23 |  |
| **[Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](http://arxiv.org/abs/2510.16290v1)** | 2025-10-18 |  |
| **[Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](http://arxiv.org/abs/2510.14896v1)** | 2025-10-16 |  |
| **[HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection](http://arxiv.org/abs/2509.22544v2)** | 2025-10-16 | <details><summary>The s...</summary><p>The submission was made prematurely. The authors plan to resubmit under the supervision of the corresponding author</p></details> |
| **[Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](http://arxiv.org/abs/2510.02155v1)** | 2025-10-02 | <details><summary>14 pa...</summary><p>14 pages, video anomaly detection</p></details> |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](http://arxiv.org/abs/2511.02427v1)** | 2025-11-04 | <details><summary>15 pa...</summary><p>15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18 DECEMBER 2025</p></details> |
| **[VidText: Towards Comprehensive Evaluation for Video Text Understanding](http://arxiv.org/abs/2505.22810v2)** | 2025-11-03 |  |
| **[Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](http://arxiv.org/abs/2505.18079v4)** | 2025-11-03 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[Aligning Effective Tokens with Video Anomaly in Large Language Models](http://arxiv.org/abs/2508.06350v2)** | 2025-11-03 |  |
| **[Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](http://arxiv.org/abs/2511.00916v1)** | 2025-11-02 |  |
| **[LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts](http://arxiv.org/abs/2505.13928v2)** | 2025-11-02 |  |
| **[VideoExplorer: Think With Videos For Agentic Long-Video Understanding](http://arxiv.org/abs/2506.10821v6)** | 2025-11-01 |  |
| **[LongCat-Flash-Omni Technical Report](http://arxiv.org/abs/2511.00279v1)** | 2025-10-31 |  |
| **[Symmetric Entropy-Constrained Video Coding for Machines](http://arxiv.org/abs/2510.15347v2)** | 2025-10-31 | <details><summary>This ...</summary><p>This paper is submitted to the IEEE Transactions</p></details> |
| **[FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](http://arxiv.org/abs/2511.00141v1)** | 2025-10-31 |  |
| **[VRoPE: Rotary Position Embedding for Video Large Language Models](http://arxiv.org/abs/2502.11664v4)** | 2025-10-31 | <details><summary>EMNLP...</summary><p>EMNLP 2025 Main Camera Ready</p></details> |
| **[FOCUS: Efficient Keyframe Selection for Long Video Understanding](http://arxiv.org/abs/2510.27280v1)** | 2025-10-31 |  |
| **[AVA: Towards Agentic Video Analytics with Vision Language Models](http://arxiv.org/abs/2505.00254v5)** | 2025-10-31 | <details><summary>Accep...</summary><p>Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations and appendix</p></details> |
| **[Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](http://arxiv.org/abs/2510.26027v1)** | 2025-10-29 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA](http://arxiv.org/abs/2510.25332v1)** | 2025-10-29 |  |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](http://arxiv.org/abs/2511.02776v1)** | 2025-11-04 |  |
| **[Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal Adverse Weather Removal](http://arxiv.org/abs/2503.09013v2)** | 2025-11-04 |  |
| **[Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes](http://arxiv.org/abs/2511.02503v1)** | 2025-11-04 |  |
| **[MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents](http://arxiv.org/abs/2503.10809v2)** | 2025-11-04 | NeurIPS 2025 |
| **[RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning](http://arxiv.org/abs/2511.02384v1)** | 2025-11-04 |  |
| **[From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](http://arxiv.org/abs/2506.03093v2)** | 2025-11-04 | <details><summary>39th ...</summary><p>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</p></details> |
| **[The Pervasive Blind Spot: Benchmarking VLM Inference Risks on Everyday Personal Videos](http://arxiv.org/abs/2511.02367v1)** | 2025-11-04 |  |
| **[Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation](http://arxiv.org/abs/2503.18065v3)** | 2025-11-04 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Neural Networks and Learning Systems</p></details> |
| **[CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](http://arxiv.org/abs/2511.02360v1)** | 2025-11-04 |  |
| **[Grounded Vision-Language Interpreter for Integrated Task and Motion Planning](http://arxiv.org/abs/2506.03270v2)** | 2025-11-04 | <details><summary>Proje...</summary><p>Project website: https://omron-sinicx.github.io/ViLaIn-TAMP/</p></details> |
| **[LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](http://arxiv.org/abs/2511.02239v1)** | 2025-11-04 | <details><summary>Prepr...</summary><p>Preprint. Project page: https://vla2026.github.io/LACY/</p></details> |
| **[Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](http://arxiv.org/abs/2511.01831v2)** | 2025-11-04 |  |
| **[ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](http://arxiv.org/abs/2511.00511v2)** | 2025-11-04 |  |
| **[MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](http://arxiv.org/abs/2508.06534v3)** | 2025-11-04 | <details><summary>ACM M...</summary><p>ACM MM 2025 Most Popular Demo Award</p></details> |
| **[DRIP: Dynamic patch Reduction via Interpretable Pooling](http://arxiv.org/abs/2510.25067v2)** | 2025-11-04 | Need more refinement |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](http://arxiv.org/abs/2511.02834v1)** | 2025-11-04 | <details><summary>16 pa...</summary><p>16 pages, 7 figures, 14 tables. Under Review</p></details> |
| **[When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](http://arxiv.org/abs/2511.02794v1)** | 2025-11-04 | <details><summary>Accep...</summary><p>Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop, NeurIPS 2025</p></details> |
| **[When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought](http://arxiv.org/abs/2511.02779v1)** | 2025-11-04 | 28 pages, 15 figures |
| **[LLEXICORP: End-user Explainability of Convolutional Neural Networks](http://arxiv.org/abs/2511.02720v1)** | 2025-11-04 |  |
| **[Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning](http://arxiv.org/abs/2508.08039v3)** | 2025-11-04 | preprint |
| **[GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution](http://arxiv.org/abs/2505.21375v2)** | 2025-11-04 | <details><summary>Neurl...</summary><p>NeurlPS 2025 Spotlight</p></details> |
| **[UniChange: Unifying Change Detection with Multimodal Large Language Model](http://arxiv.org/abs/2511.02607v1)** | 2025-11-04 |  |
| **[Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions](http://arxiv.org/abs/2510.27195v2)** | 2025-11-04 | ICCV2025 Workshop |
| **[Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](http://arxiv.org/abs/2510.16888v3)** | 2025-11-04 |  |
| **[ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension](http://arxiv.org/abs/2511.02415v1)** | 2025-11-04 | <details><summary>23 pa...</summary><p>23 pages, EMNLP25 Accepted</p></details> |
| **[InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models](http://arxiv.org/abs/2510.11341v2)** | 2025-11-04 |  |
| **[SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](http://arxiv.org/abs/2511.02280v1)** | 2025-11-04 |  |
| **[When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](http://arxiv.org/abs/2511.02243v1)** | 2025-11-04 | 19 pages |
| **[Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models](http://arxiv.org/abs/2511.02182v1)** | 2025-11-04 | <details><summary>1st p...</summary><p>1st place winner of Grounded Videoqa track at the ICCV2025 Perception Test</p></details> |
| **[DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning](http://arxiv.org/abs/2505.20241v3)** | 2025-11-04 | <details><summary>28 pa...</summary><p>28 pages, 10 figures, to appear in NeurIPS 2025 (Conference on Neural Information Processing Systems)</p></details> |

