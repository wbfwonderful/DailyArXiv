---
title: Latest 15 Papers - April 02, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis](http://arxiv.org/abs/2503.21904v1)** | 2025-03-27 | 13 pages |
| **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)** | 2025-03-27 | <details><summary>Accpe...</summary><p>Accpeted by ICME 2025</p></details> |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)** | 2025-03-27 |  |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v2)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted in CVPR 2025</p></details> |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v3)** | 2025-03-26 | <details><summary>Revis...</summary><p>Revised to ACM Computing Surveys, under review, for more information and supplementary material, please see https://github.com/fdjingliu/NSVAD</p></details> |
| **[Video Anomaly Detection with Contours -- A Study](http://arxiv.org/abs/2503.19588v1)** | 2025-03-25 |  |
| **[CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2503.18808v1)** | 2025-03-24 | <details><summary>Accep...</summary><p>Accepted for publication by IEEE Transactions on Image Processing</p></details> |
| **[Anomize: Better Open Vocabulary Video Anomaly Detection](http://arxiv.org/abs/2503.18094v1)** | 2025-03-23 |  |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |
| **[Language-guided Open-world Video Anomaly Detection](http://arxiv.org/abs/2503.13160v1)** | 2025-03-17 |  |
| **[UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks](http://arxiv.org/abs/2503.12905v1)** | 2025-03-17 | <details><summary>Accep...</summary><p>Accepted by AAAI 2025</p></details> |
| **[Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos](http://arxiv.org/abs/2503.07799v1)** | 2025-03-10 |  |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Structured Keywords](http://arxiv.org/abs/2503.10653v1)** | 2025-03-07 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](http://arxiv.org/abs/2503.24376v1)** | 2025-03-31 | <details><summary>Techn...</summary><p>Technical Report (In Progress); Code released at: https://github.com/TencentARC/SEED-Bench-R1</p></details> |
| **[DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description](http://arxiv.org/abs/2503.24096v1)** | 2025-03-31 |  |
| **[H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](http://arxiv.org/abs/2503.24008v1)** | 2025-03-31 |  |
| **[A SAT-centered XAI method for Deep Learning based Video Understanding](http://arxiv.org/abs/2503.23870v1)** | 2025-03-31 |  |
| **[Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations](http://arxiv.org/abs/2503.19706v2)** | 2025-03-31 | <details><summary>CVPR ...</summary><p>CVPR 2025 Camera-ready, 18 pages, 7 figures, 9 tables</p></details> |
| **[STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training](http://arxiv.org/abs/2412.00161v2)** | 2025-03-30 |  |
| **[VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment](http://arxiv.org/abs/2406.10889v2)** | 2025-03-30 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025. Project Page, see https://katha-ai.github.io/projects/velociti</p></details> |
| **[CA^2ST: Cross-Attention in Audio, Space, and Time for Holistic Video Recognition](http://arxiv.org/abs/2503.23447v1)** | 2025-03-30 | <details><summary>27 pa...</summary><p>27 pages including appendix, TPAMI under review</p></details> |
| **[MMAD: Multi-label Micro-Action Detection in Videos](http://arxiv.org/abs/2407.05311v2)** | 2025-03-30 |  |
| **[VideoSAVi: Self-Aligned Video Language Models without Human Supervision](http://arxiv.org/abs/2412.00624v2)** | 2025-03-30 |  |
| **[OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts](http://arxiv.org/abs/2503.22952v1)** | 2025-03-29 | <details><summary>To ap...</summary><p>To appear at CVPR 2025</p></details> |
| **[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](http://arxiv.org/abs/2503.12496v2)** | 2025-03-28 |  |
| **[Frame-Voyager: Learning to Query Frames for Video Large Language Models](http://arxiv.org/abs/2410.03226v4)** | 2025-03-28 | <details><summary>ICLR ...</summary><p>ICLR 2025, Camera-ready Version</p></details> |
| **[ReWind: Understanding Long Videos with Instructed Learnable Memory](http://arxiv.org/abs/2411.15556v2)** | 2025-03-27 |  |
| **[Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model](http://arxiv.org/abs/2503.21782v1)** | 2025-03-27 | <details><summary>Techn...</summary><p>Technical Report. Project Page: https://amshaker.github.io/Mobile-VideoGPT</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[CASA: Class-Agnostic Shared Attributes in Vision-Language Models for Efficient Incremental Object Detection](http://arxiv.org/abs/2410.05804v3)** | 2025-03-31 |  |
| **[SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation](http://arxiv.org/abs/2503.24164v1)** | 2025-03-31 | 21 pages |
| **[InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models](http://arxiv.org/abs/2502.19777v2)** | 2025-03-31 |  |
| **[Cropper: Vision-Language Model for Image Cropping through In-Context Learning](http://arxiv.org/abs/2408.07790v2)** | 2025-03-31 |  |
| **[AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference](http://arxiv.org/abs/2503.23956v1)** | 2025-03-31 |  |
| **[Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models](http://arxiv.org/abs/2503.18334v2)** | 2025-03-31 | <details><summary>Accep...</summary><p>Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in the Wild</p></details> |
| **[HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment](http://arxiv.org/abs/2503.23907v1)** | 2025-03-31 |  |
| **[Texture or Semantics? Vision-Language Models Get Lost in Font Recognition](http://arxiv.org/abs/2503.23768v1)** | 2025-03-31 |  |
| **[KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language](http://arxiv.org/abs/2503.23730v1)** | 2025-03-31 | <details><summary>Accep...</summary><p>Accepted to CVPRW 2025, Workshop on Benchmarking and Expanding AI Multimodal Approaches</p></details> |
| **[From Geometry to Culture: An Iterative VLM Layout Framework for Placing Objects in Complex 3D Scene Contexts](http://arxiv.org/abs/2503.23707v1)** | 2025-03-31 |  |
| **[MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](http://arxiv.org/abs/2410.10139v2)** | 2025-03-31 | ICLR 2025 Oral |
| **[Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](http://arxiv.org/abs/2503.21817v2)** | 2025-03-31 |  |
| **[DASH: Detection and Assessment of Systematic Hallucinations of VLMs](http://arxiv.org/abs/2503.23573v1)** | 2025-03-30 |  |
| **[BiPVL-Seg: Bidirectional Progressive Vision-Language Fusion with Global-Local Alignment for Medical Image Segmentation](http://arxiv.org/abs/2503.23534v1)** | 2025-03-30 |  |
| **[Re-Aligning Language to Visual Objects with an Agentic Workflow](http://arxiv.org/abs/2503.23508v1)** | 2025-03-30 | <details><summary>33 pa...</summary><p>33 pages, 20 figures, 17 tables, ICLR 2025</p></details> |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)** | 2025-03-31 | <details><summary>Proje...</summary><p>Project Page: https://sqwu.top/Any2Cap/</p></details> |
| **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](http://arxiv.org/abs/2503.24376v1)** | 2025-03-31 | <details><summary>Techn...</summary><p>Technical Report (In Progress); Code released at: https://github.com/TencentARC/SEED-Bench-R1</p></details> |
| **[H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](http://arxiv.org/abs/2503.24008v1)** | 2025-03-31 |  |
| **[Exploring In-Context Learning Capabilities of ChatGPT for Pathological Speech Detection](http://arxiv.org/abs/2503.23873v1)** | 2025-03-31 | <details><summary>submi...</summary><p>submitted to EUSIPCO 2025</p></details> |
| **[OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training](http://arxiv.org/abs/2503.23830v1)** | 2025-03-31 |  |
| **[EmoVerse: Exploring Multimodal Large Language Models for Sentiment and Emotion Understanding](http://arxiv.org/abs/2412.08049v3)** | 2025-03-31 |  |
| **[XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?](http://arxiv.org/abs/2503.23771v1)** | 2025-03-31 | <details><summary>It ha...</summary><p>It has been accepted by CVPR2025</p></details> |
| **[STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](http://arxiv.org/abs/2503.23765v1)** | 2025-03-31 |  |
| **[AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization](http://arxiv.org/abs/2503.23733v1)** | 2025-03-31 | CVPR 2025 |
| **[HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation](http://arxiv.org/abs/2503.23715v1)** | 2025-03-31 | CVPR 2025 |
| **[Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](http://arxiv.org/abs/2503.21817v2)** | 2025-03-31 |  |
| **[Context-Independent OCR with Multimodal LLMs: Effects of Image Resolution and Visual Complexity](http://arxiv.org/abs/2503.23667v1)** | 2025-03-31 |  |
| **[Will Pre-Training Ever End? A First Step Toward Next-Generation Foundation MLLMs via Self-Improving Systematic Cognition](http://arxiv.org/abs/2503.12303v5)** | 2025-03-31 | <details><summary>40 pa...</summary><p>40 pages. Preprint, work in progress</p></details> |
| **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v2)** | 2025-03-30 |  |
| **[OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation](http://arxiv.org/abs/2411.18499v3)** | 2025-03-30 | <details><summary>53 pa...</summary><p>53 pages, 19 figures, accepted by CVPR 2025</p></details> |

