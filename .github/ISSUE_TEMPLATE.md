---
title: Latest 15 Papers - April 25, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches](http://arxiv.org/abs/2504.14753v1)** | 2025-04-20 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Image Processing (TIP)</p></details> |
| **[EventVAD: Training-Free Event-Aware Video Anomaly Detection](http://arxiv.org/abs/2504.13092v1)** | 2025-04-17 |  |
| **[SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model](http://arxiv.org/abs/2504.10320v1)** | 2025-04-14 |  |
| **[AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection](http://arxiv.org/abs/2504.04495v1)** | 2025-04-06 | <details><summary>11 pa...</summary><p>11 pages, 4 figures, 6 tables</p></details> |
| **[From Explicit Rules to Implicit Reasoning in Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2410.21991v6)** | 2025-04-06 | <details><summary>This ...</summary><p>This manuscript has been submitted to IEEE Transactions on Circuits and Systems for Video Technology and is under consideration for publication, with potential copyright transfer in the future</p></details> |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v4)** | 2025-04-03 | <details><summary>Accep...</summary><p>Accepted to ACM Computing Surveys. For more information and supplementary material, please visit https://github.com/fdjingliu/NSVAD</p></details> |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v3)** | 2025-03-31 | <details><summary>Accep...</summary><p>Accepted in CVPR 2025</p></details> |
| **[AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis](http://arxiv.org/abs/2503.21904v1)** | 2025-03-27 | 13 pages |
| **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)** | 2025-03-27 | <details><summary>Accpe...</summary><p>Accpeted by ICME 2025</p></details> |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)** | 2025-03-27 |  |
| **[Video Anomaly Detection with Contours -- A Study](http://arxiv.org/abs/2503.19588v1)** | 2025-03-25 |  |
| **[CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2503.18808v1)** | 2025-03-24 | <details><summary>Accep...</summary><p>Accepted for publication by IEEE Transactions on Image Processing</p></details> |
| **[Anomize: Better Open Vocabulary Video Anomaly Detection](http://arxiv.org/abs/2503.18094v1)** | 2025-03-23 |  |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering](http://arxiv.org/abs/2406.00622v2)** | 2025-04-23 | <details><summary>ICLR ...</summary><p>ICLR 2025 accepted paper. Project url: https://xingruiwang.github.io/projects/DynSuperCLEVR/</p></details> |
| **[MR. Video: "MapReduce" is the Principle for Long Video Understanding](http://arxiv.org/abs/2504.16082v1)** | 2025-04-22 | Preprint |
| **[ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](http://arxiv.org/abs/2504.15921v1)** | 2025-04-22 |  |
| **[Vidi: Large Multimodal Models for Video Understanding and Editing](http://arxiv.org/abs/2504.15681v1)** | 2025-04-22 |  |
| **[IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](http://arxiv.org/abs/2504.15415v1)** | 2025-04-21 |  |
| **[Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](http://arxiv.org/abs/2504.15271v1)** | 2025-04-21 |  |
| **[An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](http://arxiv.org/abs/2504.15270v1)** | 2025-04-21 |  |
| **[Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](http://arxiv.org/abs/2504.02438v3)** | 2025-04-21 |  |
| **[PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild](http://arxiv.org/abs/2504.11326v2)** | 2025-04-21 | <details><summary>Works...</summary><p>Workshop Page: https://pvuw.github.io/. arXiv admin note: text overlap with arXiv:2504.00476, arXiv:2504.05178</p></details> |
| **[OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](http://arxiv.org/abs/2504.14692v1)** | 2025-04-20 |  |
| **[Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection](http://arxiv.org/abs/2504.14553v1)** | 2025-04-20 |  |
| **[Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](http://arxiv.org/abs/2504.14526v1)** | 2025-04-20 |  |
| **[ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](http://arxiv.org/abs/2504.14432v1)** | 2025-04-20 |  |
| **[How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?](http://arxiv.org/abs/2504.14391v1)** | 2025-04-19 |  |
| **[3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](http://arxiv.org/abs/2412.07825v2)** | 2025-04-19 | <details><summary>Proje...</summary><p>Project page: https://3dsrbench.github.io</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](http://arxiv.org/abs/2411.18203v5)** | 2025-04-23 | 16 pages, 11 figures |
| **[MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion](http://arxiv.org/abs/2503.20698v3)** | 2025-04-23 |  |
| **[V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations](http://arxiv.org/abs/2504.16727v1)** | 2025-04-23 |  |
| **[Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models](http://arxiv.org/abs/2407.17914v2)** | 2025-04-23 |  |
| **[Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization](http://arxiv.org/abs/2504.13460v2)** | 2025-04-23 |  |
| **[Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](http://arxiv.org/abs/2504.13945v3)** | 2025-04-23 | <details><summary>12 pa...</summary><p>12 pages, 5 figures, 5 Tables</p></details> |
| **[Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes](http://arxiv.org/abs/2504.16538v1)** | 2025-04-23 | <details><summary>25 pa...</summary><p>25 pages, 6 figures in main paper, 6 figures in appendices</p></details> |
| **[TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance](http://arxiv.org/abs/2504.16505v1)** | 2025-04-23 |  |
| **[CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts](http://arxiv.org/abs/2311.16445v6)** | 2025-04-23 | <details><summary>Accep...</summary><p>Accepted as a conference paper at ECCV 2024</p></details> |
| **[FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing](http://arxiv.org/abs/2504.16433v1)** | 2025-04-23 |  |
| **[Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering](http://arxiv.org/abs/2406.00622v2)** | 2025-04-23 | <details><summary>ICLR ...</summary><p>ICLR 2025 accepted paper. Project url: https://xingruiwang.github.io/projects/DynSuperCLEVR/</p></details> |
| **[CLIP-IT: CLIP-based Pairing for Histology Images Classification](http://arxiv.org/abs/2504.16181v1)** | 2025-04-22 |  |
| **[MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](http://arxiv.org/abs/2504.16083v1)** | 2025-04-22 |  |
| **[MR. Video: "MapReduce" is the Principle for Long Video Understanding](http://arxiv.org/abs/2504.16082v1)** | 2025-04-22 | Preprint |
| **[Describe Anything: Detailed Localized Image and Video Captioning](http://arxiv.org/abs/2504.16072v1)** | 2025-04-22 | <details><summary>Proje...</summary><p>Project page: https://describe-anything.github.io/</p></details> |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks](http://arxiv.org/abs/2405.07229v2)** | 2025-04-23 | <details><summary>Accep...</summary><p>Accepted by the Information Fusion Journal</p></details> |
| **[Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](http://arxiv.org/abs/2504.16427v1)** | 2025-04-23 | 23 pages, 5 figures |
| **[ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos](http://arxiv.org/abs/2503.12542v2)** | 2025-04-23 |  |
| **[Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs](http://arxiv.org/abs/2504.16323v1)** | 2025-04-22 | <details><summary>Accep...</summary><p>Accepted to CHI 2025, in press. See the project page at mediacontentatlas.github.io</p></details> |
| **[Multimodal Situational Safety](http://arxiv.org/abs/2410.06172v2)** | 2025-04-22 | <details><summary>ICLR ...</summary><p>ICLR 2025 Camera Ready</p></details> |
| **[BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger](http://arxiv.org/abs/2408.09093v3)** | 2025-04-22 |  |
| **[TextSquare: Scaling up Text-Centric Visual Instruction Tuning](http://arxiv.org/abs/2404.12803v2)** | 2025-04-22 |  |
| **[FaceInsight: A Multimodal Large Language Model for Face Perception](http://arxiv.org/abs/2504.15624v1)** | 2025-04-22 |  |
| **[AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization](http://arxiv.org/abs/2504.15619v1)** | 2025-04-22 |  |
| **[Manipulating Multimodal Agents via Cross-Modal Prompt Injection](http://arxiv.org/abs/2504.14348v2)** | 2025-04-22 | 17 pages, 5 figures |
| **[See or Recall: A Sanity Check for the Role of Vision in Solving Visualization Question Answer Tasks with Multimodal LLMs](http://arxiv.org/abs/2504.09809v2)** | 2025-04-21 |  |
| **[IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](http://arxiv.org/abs/2504.15415v1)** | 2025-04-21 |  |
| **[Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](http://arxiv.org/abs/2504.16134v1)** | 2025-04-21 |  |
| **[VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](http://arxiv.org/abs/2504.15279v1)** | 2025-04-21 | <details><summary>Code,...</summary><p>Code, data, and baselines are available at https://visulogic-benchmark.github.io/VisuLogic</p></details> |
| **[STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](http://arxiv.org/abs/2503.23765v3)** | 2025-04-21 |  |

