---
title: Latest 15 Papers - February 24, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](http://arxiv.org/abs/2410.01506v3)** | 2025-02-05 | <details><summary>Accep...</summary><p>Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[An Attribute-based Method for Video Anomaly Detection](http://arxiv.org/abs/2212.00789v2)** | 2025-01-26 | <details><summary>TMLR ...</summary><p>TMLR 2025. Our code is available at https://github.com/talreiss/Accurate-Interpretable-VAD</p></details> |
| **[Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2406.02831v2)** | 2025-01-18 |  |
| **[ComplexVAD: Detecting Interaction Anomalies in Video](http://arxiv.org/abs/2501.09733v1)** | 2025-01-16 | <details><summary>16 pa...</summary><p>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</p></details> |
| **[Detecting Contextual Anomalies by Discovering Consistent Spatial Regions](http://arxiv.org/abs/2501.08470v1)** | 2025-01-14 |  |
| **[Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive Knowledge Graph Learning](http://arxiv.org/abs/2411.09072v2)** | 2025-01-14 | <details><summary>Accep...</summary><p>Accepted to DATE 2025</p></details> |
| **[Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2412.20455v1)** | 2024-12-29 | <details><summary>Accep...</summary><p>Accepted to CVPR'24 MULA Workshop</p></details> |
| **[Exploring the Magnitude-Shape Plot Framework for Anomaly Detection in Crowded Video Scenes](http://arxiv.org/abs/2412.20363v1)** | 2024-12-29 | <details><summary>21 pa...</summary><p>21 pages, 4 figures, 10 tables</p></details> |
| **[Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems](http://arxiv.org/abs/2412.20201v1)** | 2024-12-28 | <details><summary>IEEE ...</summary><p>IEEE TETC-CS (Under review)</p></details> |
| **[STNMamba: Mamba-based Spatial-Temporal Normality Learning for Video Anomaly Detection](http://arxiv.org/abs/2412.20084v1)** | 2024-12-28 |  |
| **[Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight](http://arxiv.org/abs/2412.18298v1)** | 2024-12-24 | Research report |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v1)** | 2024-12-23 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Motion and Appearance Guided Patch Diffusion Model](http://arxiv.org/abs/2412.09026v1)** | 2024-12-12 | Accept by AAAI2025 |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v1)** | 2024-12-04 |  |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v1)** | 2024-12-02 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AVD2: Accident Video Diffusion for Accident Video Description](http://arxiv.org/abs/2502.14801v1)** | 2025-02-20 | <details><summary>ICRA ...</summary><p>ICRA 2025, Project Page: https://an-answer-tree.github.io/</p></details> |
| **[AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](http://arxiv.org/abs/2410.03051v2)** | 2025-02-20 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025. Code, docs, weight, benchmark and training data are all avaliable at https://rese1f.github.io/aurora-web/</p></details> |
| **[MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos](http://arxiv.org/abs/2502.12558v1)** | 2025-02-18 |  |
| **[iMOVE: Instance-Motion-Aware Video Understanding](http://arxiv.org/abs/2502.11594v2)** | 2025-02-18 |  |
| **[Understanding Long Videos with Multimodal Language Models](http://arxiv.org/abs/2403.16998v3)** | 2025-02-17 | <details><summary>Code ...</summary><p>Code available at https://github.com/kahnchana/mvu</p></details> |
| **[video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model](http://arxiv.org/abs/2502.11775v1)** | 2025-02-17 |  |
| **[VRoPE: Rotary Position Embedding for Video Large Language Models](http://arxiv.org/abs/2502.11664v1)** | 2025-02-17 | 10 pages |
| **[Do Language Models Understand Time?](http://arxiv.org/abs/2412.13845v2)** | 2025-02-16 | <details><summary>Accep...</summary><p>Accepted for publication in the Companion Proceedings of the ACM Web Conference (WWW Companion 2025)</p></details> |
| **[SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](http://arxiv.org/abs/2502.10810v1)** | 2025-02-15 | <details><summary>ICLR ...</summary><p>ICLR 2025 Accept (Spotlight)</p></details> |
| **[Semantics-aware Test-time Adaptation for 3D Human Pose Estimation](http://arxiv.org/abs/2502.10724v1)** | 2025-02-15 | 10 pages, 4 figures |
| **[VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks](http://arxiv.org/abs/2410.19100v3)** | 2025-02-15 |  |
| **[Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering](http://arxiv.org/abs/2502.09573v2)** | 2025-02-14 |  |
| **[TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning](http://arxiv.org/abs/2410.19702v2)** | 2025-02-12 | Accepted by ICLR2025 |
| **[A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems](http://arxiv.org/abs/2502.06581v2)** | 2025-02-12 |  |
| **[CoS: Chain-of-Shot Prompting for Long Video Understanding](http://arxiv.org/abs/2502.06428v2)** | 2025-02-11 | <details><summary>A tra...</summary><p>A training-free test-time optimisation approach for long video understanding</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation](http://arxiv.org/abs/2502.14846v1)** | 2025-02-20 | <details><summary>20 pa...</summary><p>20 pages, 19 figures, 9 tables, website: https://yueyang1996.github.io/cosyn/</p></details> |
| **[LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models](http://arxiv.org/abs/2502.14834v1)** | 2025-02-20 |  |
| **[SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](http://arxiv.org/abs/2502.14786v1)** | 2025-02-20 | <details><summary>Model...</summary><p>Model checkpoints are available at https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md</p></details> |
| **[ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting](http://arxiv.org/abs/2502.14780v1)** | 2025-02-20 | <details><summary>12 pa...</summary><p>12 pages, 7 figures, 3 tables</p></details> |
| **[HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](http://arxiv.org/abs/2502.14744v1)** | 2025-02-20 |  |
| **[NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization](http://arxiv.org/abs/2502.14638v1)** | 2025-02-20 |  |
| **[Noisy Test-Time Adaptation in Vision-Language Models](http://arxiv.org/abs/2502.14604v1)** | 2025-02-20 | ICLR 2025 |
| **[PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models](http://arxiv.org/abs/2502.14504v1)** | 2025-02-20 | 12 pages, 8 figures |
| **[How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation](http://arxiv.org/abs/2502.14486v1)** | 2025-02-20 |  |
| **[Evaluating Precise Geolocation Inference Capabilities of Vision Language Models](http://arxiv.org/abs/2502.14412v1)** | 2025-02-20 | <details><summary>AAAI ...</summary><p>AAAI 2025 Workshop DATASAFE</p></details> |
| **[Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder](http://arxiv.org/abs/2411.05195v2)** | 2025-02-20 | 17 pages, 3 figures |
| **[On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective](http://arxiv.org/abs/2502.14296v1)** | 2025-02-20 |  |
| **[MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal Scientific Understanding](http://arxiv.org/abs/2407.04903v3)** | 2025-02-20 | <details><summary>Code ...</summary><p>Code and data are available at https://github.com/Leezekun/MMSci</p></details> |
| **[Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation](http://arxiv.org/abs/2502.14254v1)** | 2025-02-20 |  |
| **[Enhancing Adversarial Robustness of Vision-Language Models through Low-Rank Adaptation](http://arxiv.org/abs/2404.13425v3)** | 2025-02-20 |  |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder](http://arxiv.org/abs/2411.05195v2)** | 2025-02-20 | 17 pages, 3 figures |
| **[Interaction2Code: Benchmarking MLLM-based Interactive Webpage Code Generation from Interactive Prototyping](http://arxiv.org/abs/2411.03292v2)** | 2025-02-20 | 21 pages,14 figures |
| **[Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach](http://arxiv.org/abs/2502.14285v1)** | 2025-02-20 | <details><summary>14 pa...</summary><p>14 pages,8 figures,4 tables</p></details> |
| **[LOVA3: Learning to Visual Question Answering, Asking and Assessment](http://arxiv.org/abs/2405.14974v3)** | 2025-02-19 | <details><summary>NeurI...</summary><p>NeurIPS 2024. The code is available at https://github.com/showlab/LOVA3</p></details> |
| **[ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities](http://arxiv.org/abs/2502.13832v1)** | 2025-02-19 | <details><summary>18 pa...</summary><p>18 pages, 12 figures. Accepted by CHI 2025</p></details> |
| **[FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](http://arxiv.org/abs/2412.12567v2)** | 2025-02-19 |  |
| **[Contrastive Localized Language-Image Pre-Training](http://arxiv.org/abs/2410.02746v2)** | 2025-02-19 | Preprint |
| **[Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation](http://arxiv.org/abs/2502.13145v1)** | 2025-02-18 | <details><summary>Code ...</summary><p>Code and model are available at https://github.com/hustvl/mmMamba</p></details> |
| **[SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models](http://arxiv.org/abs/2502.13059v1)** | 2025-02-18 |  |
| **[Gesture-Aware Zero-Shot Speech Recognition for Patients with Language Disorders](http://arxiv.org/abs/2502.13983v1)** | 2025-02-18 |  |
| **[Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality](http://arxiv.org/abs/2410.04780v2)** | 2025-02-18 | <details><summary>Accep...</summary><p>Accepted by The Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[Towards Text-Image Interleaved Retrieval](http://arxiv.org/abs/2502.12799v1)** | 2025-02-18 | 16 pages, 14 figures |
| **[RedundancyLens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only MLLMs](http://arxiv.org/abs/2501.19036v2)** | 2025-02-18 |  |
| **[Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!](http://arxiv.org/abs/2501.10674v2)** | 2025-02-18 | <details><summary>Our d...</summary><p>Our dataset can be found at \url{https://huggingface.co/datasets/fazliimam/temporal-vqa}</p></details> |
| **[Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in Visual Instruction Tuning](http://arxiv.org/abs/2502.12635v1)** | 2025-02-18 |  |

