---
title: Latest 15 Papers - March 31, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)** | 2025-03-27 | <details><summary>Accpe...</summary><p>Accpeted by ICME 2025</p></details> |
| **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)** | 2025-03-27 |  |
| **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v2)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted in CVPR 2025</p></details> |
| **[Networking Systems for Video Anomaly Detection: A Tutorial and Survey](http://arxiv.org/abs/2405.10347v3)** | 2025-03-26 | <details><summary>Revis...</summary><p>Revised to ACM Computing Surveys, under review, for more information and supplementary material, please see https://github.com/fdjingliu/NSVAD</p></details> |
| **[Video Anomaly Detection with Contours -- A Study](http://arxiv.org/abs/2503.19588v1)** | 2025-03-25 |  |
| **[CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos](http://arxiv.org/abs/2503.18808v1)** | 2025-03-24 | <details><summary>Accep...</summary><p>Accepted for publication by IEEE Transactions on Image Processing</p></details> |
| **[Anomize: Better Open Vocabulary Video Anomaly Detection](http://arxiv.org/abs/2503.18094v1)** | 2025-03-23 |  |
| **[Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark](http://arxiv.org/abs/2408.14329v2)** | 2025-03-19 |  |
| **[Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer](http://arxiv.org/abs/2408.15185v2)** | 2025-03-17 |  |
| **[Language-guided Open-world Video Anomaly Detection](http://arxiv.org/abs/2503.13160v1)** | 2025-03-17 |  |
| **[UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks](http://arxiv.org/abs/2503.12905v1)** | 2025-03-17 | <details><summary>Accep...</summary><p>Accepted by AAAI 2025</p></details> |
| **[Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos](http://arxiv.org/abs/2503.07799v1)** | 2025-03-10 |  |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Structured Keywords](http://arxiv.org/abs/2503.10653v1)** | 2025-03-07 |  |
| **[AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](http://arxiv.org/abs/2503.04504v1)** | 2025-03-06 |  |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model](http://arxiv.org/abs/2503.21782v1)** | 2025-03-27 | <details><summary>Techn...</summary><p>Technical Report. Project Page: https://amshaker.github.io/Mobile-VideoGPT</p></details> |
| **[OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](http://arxiv.org/abs/2501.05510v2)** | 2025-03-27 | CVPR 2025 |
| **[SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](http://arxiv.org/abs/2503.18943v2)** | 2025-03-27 | Technical report |
| **[BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](http://arxiv.org/abs/2503.21483v1)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **[LongViTU: Instruction Tuning for Long-Form Video Understanding](http://arxiv.org/abs/2501.05037v2)** | 2025-03-27 |  |
| **[M-LLM Based Video Frame Selection for Efficient Video Understanding](http://arxiv.org/abs/2502.19680v2)** | 2025-03-26 |  |
| **[From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment](http://arxiv.org/abs/2503.20472v1)** | 2025-03-26 |  |
| **[Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video Understanding](http://arxiv.org/abs/2503.20362v1)** | 2025-03-26 |  |
| **[Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection](http://arxiv.org/abs/2503.03562v3)** | 2025-03-26 | Accepted by CVPR25 |
| **[Progress-Aware Video Frame Captioning](http://arxiv.org/abs/2412.02071v2)** | 2025-03-26 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025, Project website: https://vision.cs.utexas.edu/projects/ProgressCaptioner/</p></details> |
| **[HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding](http://arxiv.org/abs/2501.01645v2)** | 2025-03-26 | <details><summary>Accep...</summary><p>Accepted to ICME 2025</p></details> |
| **[LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living](http://arxiv.org/abs/2406.09390v3)** | 2025-03-25 | <details><summary>CVPR ...</summary><p>CVPR 2025 Camera Ready</p></details> |
| **[From My View to Yours: Ego-Augmented Learning in Large Vision Language Models for Understanding Exocentric Daily Living Activities](http://arxiv.org/abs/2501.05711v2)** | 2025-03-25 |  |
| **[ACVUBench: Audio-Centric Video Understanding Benchmark](http://arxiv.org/abs/2503.19951v1)** | 2025-03-25 |  |
| **[PAVE: Patching and Adapting Video Large Language Models](http://arxiv.org/abs/2503.19794v1)** | 2025-03-25 | <details><summary>CVPR2...</summary><p>CVPR2025 Camera Ready</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval](http://arxiv.org/abs/2502.15682v2)** | 2025-03-27 |  |
| **[Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck](http://arxiv.org/abs/2503.21757v1)** | 2025-03-27 |  |
| **[BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs](http://arxiv.org/abs/2407.03314v2)** | 2025-03-27 |  |
| **[Vision language models are blind: Failing to translate detailed visual features into words](http://arxiv.org/abs/2407.06581v6)** | 2025-03-27 |  |
| **[GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI](http://arxiv.org/abs/2411.14522v2)** | 2025-03-27 |  |
| **[Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving](http://arxiv.org/abs/2503.21505v1)** | 2025-03-27 |  |
| **[BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](http://arxiv.org/abs/2503.21483v1)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |
| **[Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models](http://arxiv.org/abs/2503.21435v1)** | 2025-03-27 |  |
| **[RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models](http://arxiv.org/abs/2503.19654v2)** | 2025-03-27 |  |
| **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v2)** | 2025-03-27 |  |
| **[Cultivating Game Sense for Yourself: Making VLMs Gaming Experts](http://arxiv.org/abs/2503.21263v1)** | 2025-03-27 |  |
| **[ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom](http://arxiv.org/abs/2410.14138v2)** | 2025-03-27 |  |
| **[LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](http://arxiv.org/abs/2503.21227v1)** | 2025-03-27 | Preprint |
| **[VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models via Voxel Representation](http://arxiv.org/abs/2503.21214v1)** | 2025-03-27 |  |
| **[FakeReasoning: Towards Generalizable Forgery Detection and Reasoning](http://arxiv.org/abs/2503.21210v1)** | 2025-03-27 |  |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Do Multimodal Large Language Models See Like Humans?](http://arxiv.org/abs/2412.09603v2)** | 2025-03-27 | <details><summary>Proje...</summary><p>Project page: https://jiaying.link/HVSBench/</p></details> |
| **[Video-R1: Reinforcing Video Reasoning in MLLMs](http://arxiv.org/abs/2503.21776v1)** | 2025-03-27 | <details><summary>Proje...</summary><p>Project page: https://github.com/tulerfeng/Video-R1</p></details> |
| **[OmniBench: Towards The Future of Universal Omni-Language Models](http://arxiv.org/abs/2409.15272v4)** | 2025-03-27 |  |
| **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v1)** | 2025-03-27 |  |
| **[FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs](http://arxiv.org/abs/2503.21457v1)** | 2025-03-27 | Accepted by CVPR2025 |
| **[Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](http://arxiv.org/abs/2412.00493v2)** | 2025-03-27 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |
| **[What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models](http://arxiv.org/abs/2405.15668v4)** | 2025-03-27 |  |
| **[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](http://arxiv.org/abs/2503.21307v1)** | 2025-03-27 |  |
| **[Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy](http://arxiv.org/abs/2503.20673v2)** | 2025-03-27 |  |
| **[Online Reasoning Video Segmentation with Just-in-Time Digital Twins](http://arxiv.org/abs/2503.21056v1)** | 2025-03-27 |  |
| **[ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and Prediction](http://arxiv.org/abs/2503.20978v1)** | 2025-03-26 | <details><summary>Accep...</summary><p>Accepted to MM4SG Workshop at The Web Conference 2025</p></details> |
| **[MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams](http://arxiv.org/abs/2503.20745v1)** | 2025-03-26 |  |
| **[Vision-Amplified Semantic Entropy for Hallucination Detection in Medical Visual Question Answering](http://arxiv.org/abs/2503.20504v1)** | 2025-03-26 | 11 pages, 2 figures |
| **[MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation](http://arxiv.org/abs/2503.20384v1)** | 2025-03-26 |  |
| **[Dynamic Pyramid Network for Efficient Multimodal Large Language Model](http://arxiv.org/abs/2503.20322v1)** | 2025-03-26 |  |

