---
title: Latest 15 Papers - December 16, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845v1)** | 2025-12-07 |  |
| **[CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840v1)** | 2025-12-07 | <details><summary>Accep...</summary><p>Accepted to WACV 2026</p></details> |
| **[AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](https://arxiv.org/abs/2503.04504v4)** | 2025-12-05 | <details><summary>Accep...</summary><p>Accepted to WACV 2026</p></details> |
| **[Video Anomaly Detection with Semantics-Aware Information Bottleneck](https://arxiv.org/abs/2506.02535v3)** | 2025-12-01 |  |
| **[Pistachio: Towards Synthetic, Balanced, and Long-Form Video Anomaly Benchmarks](https://arxiv.org/abs/2511.19474v3)** | 2025-11-30 |  |
| **[HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection](https://arxiv.org/abs/2509.22544v3)** | 2025-11-29 | 25 pages, 1 figure |
| **[Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models](https://arxiv.org/abs/2511.17094v1)** | 2025-11-21 |  |
| **[RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204v1)** | 2025-11-17 | <details><summary>Accep...</summary><p>Accepted to AAAI 2026</p></details> |
| **[Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321v1)** | 2025-11-15 | <details><summary>Accep...</summary><p>Accepted at the Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</p></details> |
| **[Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation](https://arxiv.org/abs/2506.11777v2)** | 2025-11-14 |  |
| **[Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment](https://arxiv.org/abs/2511.10334v1)** | 2025-11-13 | <details><summary>Accep...</summary><p>Accepted to AAAI 2026. Code is available at https://github.com/lessiYin/DSANet</p></details> |
| **[Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524v2)** | 2025-11-05 |  |
| **[M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection](https://arxiv.org/abs/2511.05564v1)** | 2025-11-04 | IEEE VCIP 2025 |
| **[A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962v1)** | 2025-11-02 | NeurIPS 2025 poster |
| **[TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580v1)** | 2025-11-01 | 10 pages, 5 figures |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning](https://arxiv.org/abs/2512.11534v1)** | 2025-12-12 | 18 pages, 8 figures |
| **[UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models](https://arxiv.org/abs/2512.11336v1)** | 2025-12-12 | <details><summary>22 pa...</summary><p>22 pages, 13 figures, technical report</p></details> |
| **[MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation](https://arxiv.org/abs/2512.10945v1)** | 2025-12-11 | <details><summary>IEEE ...</summary><p>IEEE TPAMI, Project Page: https://henghuiding.com/MeViS/</p></details> |
| **[Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos](https://arxiv.org/abs/2512.10607v1)** | 2025-12-11 |  |
| **[From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](https://arxiv.org/abs/2511.02427v2)** | 2025-12-10 | <details><summary>15 pa...</summary><p>15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18 DECEMBER 2025</p></details> |
| **[Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding](https://arxiv.org/abs/2512.09354v1)** | 2025-12-10 |  |
| **[InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829v1)** | 2025-12-09 | <details><summary>16 pa...</summary><p>16 pages, 8 figures, conference or other essential info</p></details> |
| **[Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval](https://arxiv.org/abs/2512.08410v1)** | 2025-12-09 |  |
| **[Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518v2)** | 2025-12-09 | <details><summary>Accep...</summary><p>Accepted by ICCV 2025</p></details> |
| **[Harnessing Object Grounding for Time-Sensitive Video Understanding](https://arxiv.org/abs/2509.06335v2)** | 2025-12-08 | <details><summary>Accep...</summary><p>Accepted to WACV 2026</p></details> |
| **[Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747v1)** | 2025-12-08 |  |
| **[Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding](https://arxiv.org/abs/2512.07344v1)** | 2025-12-08 | <details><summary>Accep...</summary><p>Accepted by IEEE International Conference on Computer Communications 2026</p></details> |
| **[Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models](https://arxiv.org/abs/2511.23478v2)** | 2025-12-08 | <details><summary>Video...</summary><p>Video-R2 Technical Report</p></details> |
| **[TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos](https://arxiv.org/abs/2509.26360v3)** | 2025-12-08 |  |
| **[NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921v1)** | 2025-12-07 | <details><summary>Accep...</summary><p>Accepted by IEEE ICIA 2025</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet](https://arxiv.org/abs/2512.11567v1)** | 2025-12-12 | <details><summary>Submi...</summary><p>Submitted to LREC 2026</p></details> |
| **[VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing](https://arxiv.org/abs/2512.11490v1)** | 2025-12-12 | <details><summary>21 pa...</summary><p>21 pages, 7 figures, under review</p></details> |
| **[From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models](https://arxiv.org/abs/2512.10867v2)** | 2025-12-12 |  |
| **[Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction](https://arxiv.org/abs/2512.11399v1)** | 2025-12-12 |  |
| **[The N-Body Problem: Parallel Execution from Single-Person Egocentric Video](https://arxiv.org/abs/2512.11393v1)** | 2025-12-12 | <details><summary>proje...</summary><p>project webpage: https://zhifanzhu.github.io/ego-nbody</p></details> |
| **[Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522v2)** | 2025-12-12 |  |
| **[Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture](https://arxiv.org/abs/2512.11350v1)** | 2025-12-12 |  |
| **[Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315v1)** | 2025-12-12 | <details><summary>23 pa...</summary><p>23 pages, 7 figures, and 1 table</p></details> |
| **[The Finer the Better: Towards Granular-aware Open-set Domain Generalization](https://arxiv.org/abs/2511.16979v2)** | 2025-12-12 | <details><summary>9 pag...</summary><p>9 pages,3 figures,aaai2026</p></details> |
| **[Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing](https://arxiv.org/abs/2512.11275v1)** | 2025-12-12 | <details><summary>8 pag...</summary><p>8 pages, 2 figures, submitted to the 2026 IFAC World Congress</p></details> |
| **[Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy](https://arxiv.org/abs/2512.11218v1)** | 2025-12-12 |  |
| **[Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/abs/2505.17982v5)** | 2025-12-12 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025</p></details> |
| **[Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330v2)** | 2025-12-12 |  |
| **[GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396v2)** | 2025-12-12 | NeurIPS 2025 |
| **[Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context](https://arxiv.org/abs/2512.11167v1)** | 2025-12-11 | <details><summary>Accep...</summary><p>Accepted in AAAI 2025 Workshop on Reproducible AI</p></details> |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry](https://arxiv.org/abs/2512.11558v1)** | 2025-12-12 |  |
| **[HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning](https://arxiv.org/abs/2512.11534v1)** | 2025-12-12 | 18 pages, 8 figures |
| **[Reconstruction as a Bridge for Event-Based Visual Question Answering](https://arxiv.org/abs/2512.11510v1)** | 2025-12-12 |  |
| **[Exploring MLLM-Diffusion Information Transfer with MetaCanvas](https://arxiv.org/abs/2512.11464v1)** | 2025-12-12 | <details><summary>Proje...</summary><p>Project page: https://metacanvas.github.io</p></details> |
| **[Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation](https://arxiv.org/abs/2503.24379v2)** | 2025-12-12 | <details><summary>Proje...</summary><p>Project Page: https://sqwu.top/Any2Cap/</p></details> |
| **[KeyframeFace: From Text to Expressive Facial Keyframes](https://arxiv.org/abs/2512.11321v1)** | 2025-12-12 |  |
| **[SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection](https://arxiv.org/abs/2512.11215v1)** | 2025-12-12 | <details><summary>Accep...</summary><p>Accepted to WACV 2026</p></details> |
| **[VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction](https://arxiv.org/abs/2512.11099v1)** | 2025-12-11 | 8 pages |
| **[Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882v1)** | 2025-12-11 |  |
| **[LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models](https://arxiv.org/abs/2512.05391v2)** | 2025-12-11 | 20 pages |
| **[LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation](https://arxiv.org/abs/2512.10750v1)** | 2025-12-11 | Work in progress |
| **[TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection](https://arxiv.org/abs/2512.10652v1)** | 2025-12-11 |  |
| **[AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624v1)** | 2025-12-11 |  |
| **[SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence](https://arxiv.org/abs/2505.17012v2)** | 2025-12-11 | <details><summary>Techn...</summary><p>Technical Report; Project Page: https://haoningwu3639.github.io/SpatialScore</p></details> |
| **[Grounding Everything in Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2512.10554v1)** | 2025-12-11 | <details><summary>19 pa...</summary><p>19 pages, 16 figures, 12 Tables</p></details> |

