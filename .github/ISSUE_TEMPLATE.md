---
title: Latest 15 Papers - March 18, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Anomaly Detection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos](http://arxiv.org/abs/2503.07799v1)** | 2025-03-10 |  |
| **[Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](http://arxiv.org/abs/2412.17210v2)** | 2025-03-08 | <details><summary>Code ...</summary><p>Code is on https://github.com/guijiejie/DCMD-main</p></details> |
| **[Video Anomaly Detection with Structured Keywords](http://arxiv.org/abs/2503.10653v1)** | 2025-03-07 |  |
| **[AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](http://arxiv.org/abs/2503.04504v1)** | 2025-03-06 |  |
| **[Anomaly detection in non-stationary videos using time-recursive differencing network based prediction](http://arxiv.org/abs/2503.02234v1)** | 2025-03-04 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos](http://arxiv.org/abs/2503.00670v1)** | 2025-03-02 | <details><summary>Copyr...</summary><p>Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p></details> |
| **[Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](http://arxiv.org/abs/2410.01506v4)** | 2025-02-28 | <details><summary>Accep...</summary><p>Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)</p></details> |
| **[Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM](http://arxiv.org/abs/2502.18863v1)** | 2025-02-26 |  |
| **[An Attribute-based Method for Video Anomaly Detection](http://arxiv.org/abs/2212.00789v2)** | 2025-01-26 | <details><summary>TMLR ...</summary><p>TMLR 2025. Our code is available at https://github.com/talreiss/Accurate-Interpretable-VAD</p></details> |
| **[Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2406.02831v2)** | 2025-01-18 |  |
| **[ComplexVAD: Detecting Interaction Anomalies in Video](http://arxiv.org/abs/2501.09733v1)** | 2025-01-16 | <details><summary>16 pa...</summary><p>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</p></details> |
| **[Detecting Contextual Anomalies by Discovering Consistent Spatial Regions](http://arxiv.org/abs/2501.08470v1)** | 2025-01-14 |  |
| **[Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive Knowledge Graph Learning](http://arxiv.org/abs/2411.09072v2)** | 2025-01-14 | <details><summary>Accep...</summary><p>Accepted to DATE 2025</p></details> |
| **[Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2412.20455v1)** | 2024-12-29 | <details><summary>Accep...</summary><p>Accepted to CVPR'24 MULA Workshop</p></details> |
| **[Exploring the Magnitude-Shape Plot Framework for Anomaly Detection in Crowded Video Scenes](http://arxiv.org/abs/2412.20363v1)** | 2024-12-29 | <details><summary>21 pa...</summary><p>21 pages, 4 figures, 10 tables</p></details> |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers](http://arxiv.org/abs/2503.11579v1)** | 2025-03-14 | <details><summary>Proje...</summary><p>Project Page: https://tiger-ai-lab.github.io/Vamba/</p></details> |
| **[V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning](http://arxiv.org/abs/2503.11495v1)** | 2025-03-14 | <details><summary>A ben...</summary><p>A benchmark for Video Spatio-Temporal Reasoning</p></details> |
| **[VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos](http://arxiv.org/abs/2405.19209v3)** | 2025-03-14 | <details><summary>CVPR ...</summary><p>CVPR 2025; First three authors contributed equally; Project page: https://videotree2024.github.io/</p></details> |
| **[Watch and Learn: Leveraging Expert Knowledge and Language for Surgical Video Understanding](http://arxiv.org/abs/2503.11392v1)** | 2025-03-14 | <details><summary>14 pa...</summary><p>14 pages main manuscript with 3 figures; 6 pages supplementary material with 3 figures. To be presented at International Conference on Information Processing in Computer-Assisted Interventions (IPCAI 2025). To be published in International Journal of Computer Assisted Radiology and Surgery (IJCARS)</p></details> |
| **[LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free Video LLMs](http://arxiv.org/abs/2503.11205v1)** | 2025-03-14 |  |
| **[LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding](http://arxiv.org/abs/2501.05067v2)** | 2025-03-14 | 18 pages, 10 figures |
| **[On the Limitations of Vision-Language Models in Understanding Image Transforms](http://arxiv.org/abs/2503.09837v2)** | 2025-03-14 | 8 pages, 15 images |
| **[Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](http://arxiv.org/abs/2503.10742v1)** | 2025-03-13 |  |
| **[Are you Struggling? Dataset and Baselines for Struggle Determination in Assembly Videos](http://arxiv.org/abs/2402.11057v4)** | 2025-03-13 |  |
| **[FaVChat: Unlocking Fine-Grained Facial Video Understanding with Multimodal Large Language Models](http://arxiv.org/abs/2503.09158v2)** | 2025-03-13 |  |
| **[LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents](http://arxiv.org/abs/2503.10200v1)** | 2025-03-13 |  |
| **[TIME: Temporal-sensitive Multi-dimensional Instruction Tuning and Benchmarking for Video-LLMs](http://arxiv.org/abs/2503.09994v1)** | 2025-03-13 |  |
| **[Teaching VLMs to Localize Specific Objects from In-context Examples](http://arxiv.org/abs/2411.13317v2)** | 2025-03-12 |  |
| **[VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary](http://arxiv.org/abs/2503.09402v1)** | 2025-03-12 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025. Github: https://github.com/showlab/VLog</p></details> |
| **[VideoScan: Enabling Efficient Streaming Video Understanding via Frame-level Semantic Carriers](http://arxiv.org/abs/2503.09387v1)** | 2025-03-12 | 11 pages, 4 figures |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense](http://arxiv.org/abs/2503.11619v1)** | 2025-03-14 |  |
| **[Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages](http://arxiv.org/abs/2503.11609v1)** | 2025-03-14 | <details><summary>Camer...</summary><p>Camera-ready version for CVPR 2025 (w/ SuppMat, 23 pages)</p></details> |
| **[SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion](http://arxiv.org/abs/2503.11576v1)** | 2025-03-14 | 24 pages, 10 figures |
| **[Similarity-Aware Token Pruning: Your VLM but Faster](http://arxiv.org/abs/2503.11549v1)** | 2025-03-14 | <details><summary>15 pa...</summary><p>15 pages, 8 figures, 8 tables</p></details> |
| **[Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models](http://arxiv.org/abs/2503.11519v1)** | 2025-03-14 |  |
| **[Visual Adaptive Prompting for Compositional Zero-Shot Learning](http://arxiv.org/abs/2502.20292v2)** | 2025-03-14 |  |
| **[PBR3DGen: A VLM-guided Mesh Generation with High-quality PBR Texture](http://arxiv.org/abs/2503.11368v1)** | 2025-03-14 | <details><summary>Homep...</summary><p>Homepage: https://pbr3dgen1218.github.io/</p></details> |
| **[PARIC: Probabilistic Attention Regularization for Language Guided Image Classification from Pre-trained Vison Language Models](http://arxiv.org/abs/2503.11360v1)** | 2025-03-14 |  |
| **[Road Rage Reasoning with Vision-language Models (VLMs): Task Definition and Evaluation Dataset](http://arxiv.org/abs/2503.11342v1)** | 2025-03-14 |  |
| **[DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models](http://arxiv.org/abs/2503.11265v1)** | 2025-03-14 |  |
| **[A Two-Step Concept-Based Approach for Enhanced Interpretability and Trust in Skin Lesion Diagnosis](http://arxiv.org/abs/2411.05609v2)** | 2025-03-14 | <details><summary>Publi...</summary><p>Published in the Computational and Structural Biotechnology Journal</p></details> |
| **[Compound Expression Recognition via Large Vision-Language Models](http://arxiv.org/abs/2503.11241v1)** | 2025-03-14 |  |
| **[Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves](http://arxiv.org/abs/2412.11509v2)** | 2025-03-14 |  |
| **[MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders](http://arxiv.org/abs/2501.01709v2)** | 2025-03-14 | Accepted by CVPR2025 |
| **[Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention](http://arxiv.org/abs/2406.12718v3)** | 2025-03-14 | <details><summary>Accep...</summary><p>Accepted by CVPR 2025</p></details> |

## Multimodal Large Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration](http://arxiv.org/abs/2411.17686v3)** | 2025-03-14 |  |
| **[VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity](http://arxiv.org/abs/2503.11557v1)** | 2025-03-14 |  |
| **[A Framework for a Capability-driven Evaluation of Scenario Understanding for Multimodal Large Language Models in Autonomous Driving](http://arxiv.org/abs/2503.11400v1)** | 2025-03-14 | <details><summary>Submi...</summary><p>Submitted to IEEE IAVVC 2025, Under Review</p></details> |
| **[Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware](http://arxiv.org/abs/2503.11367v1)** | 2025-03-14 |  |
| **[ParGo: Bridging Vision-Language with Partial and Global Views](http://arxiv.org/abs/2408.12928v3)** | 2025-03-14 | <details><summary>Accep...</summary><p>Accepted by AAAI 2025</p></details> |
| **[LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding](http://arxiv.org/abs/2501.05067v2)** | 2025-03-14 | 18 pages, 10 figures |
| **[Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space](http://arxiv.org/abs/2503.11094v1)** | 2025-03-14 |  |
| **[OmniDiff: A Comprehensive Benchmark for Fine-grained Image Difference Captioning](http://arxiv.org/abs/2503.11093v1)** | 2025-03-14 |  |
| **[EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks](http://arxiv.org/abs/2503.11089v1)** | 2025-03-14 | technical report |
| **[BannerAgency: Advertising Banner Design with Multimodal LLM Agents](http://arxiv.org/abs/2503.11060v1)** | 2025-03-14 |  |
| **[ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding](http://arxiv.org/abs/2409.03277v3)** | 2025-03-14 |  |
| **[Learning to Inference Adaptively for Multimodal Large Language Models](http://arxiv.org/abs/2503.10905v1)** | 2025-03-13 |  |
| **[Think Before You Segment: High-Quality Reasoning Segmentation with GPT Chain of Thoughts](http://arxiv.org/abs/2503.07503v2)** | 2025-03-13 | <details><summary>Proje...</summary><p>Project page: https://cse.hkust.edu.hk/~skao/thinkfirst.html</p></details> |
| **[GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing](http://arxiv.org/abs/2503.10639v1)** | 2025-03-13 | <details><summary>Datas...</summary><p>Dataset and models are released in https://github.com/rongyaofang/GoT</p></details> |
| **[PiSA: A Self-Augmented Data Engine and Training Strategy for 3D Understanding with Large Models](http://arxiv.org/abs/2503.10529v1)** | 2025-03-13 | Technical Report |

